{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('/Dataset')\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from DressipiChallenge.Pipeline.xgboost.xgboost_tuning import XGB_hypertune, Real, Categorical, Integer\n",
    "from DressipiChallenge.Pipeline.xgboost.xgboost_utils import XGB_train, XGB_tune_test\n",
    "from DressipiChallenge.Pipeline.gradient_boosting_utils import fit_models, XGB_rerank, load_xgboost_train_df, load_xgboost_test_df, load_attributes, XGB_insert_session_feature, create_submission_XGB,\n",
    "from DressipiChallenge.Pipeline.utils import create_mapping, get_mapped_sessions_to_recommend, get_items_to_exclude\n",
    "from DressipiChallenge.Pipeline.matrices_creation import create_URM\n",
    "from DressipiChallenge.Pipeline.data_splitting import train_val_split\n",
    "from DressipiChallenge.Pipeline.data_extraction import get_dataframes\n",
    "from DressipiChallenge.Recommenders.NonPersonalizedRecommender import TopPop\n",
    "from DressipiChallenge.Recommenders.KNN.ItemKNNCFRecommender import ItemKNNCFRecommender\n",
    "from DressipiChallenge.Recommenders.GraphBased.P3alphaRecommender import P3alphaRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features_df, train_sessions_df, train_purchases_df, test_sessions_df, candidate_items_df = get_dataframes()\n",
    "\n",
    "model_classes = [P3alphaRecommender, TopPop, ItemKNNCFRecommender]\n",
    "\n",
    "models_hyp = [{'topK': 479, 'alpha': 1.1764856470188576, 'normalize_similarity': True}, {}, {'shrink': 500, 'similarity': 'asymmetric', 'feature_weighting': 'none', 'topK': 495, 'normalize': True}]\n",
    "\n",
    "config_dict = {'models_hyp': models_hyp, 'is_content_based': [False, False, False], 'model_classes' : model_classes}\n",
    "\n",
    "num_boost_round = 5\n",
    "\n",
    "xgb_model, features_to_drop, reranked_df = XGB_tune_test(\n",
    "    item_features_df=item_features_df,\n",
    "    config_dict=config_dict,\n",
    "    train_purchases_df=train_purchases_df,\n",
    "    num_boost_round=num_boost_round,\n",
    "    train_sessions_df=train_sessions_df,\n",
    "    num_trials = 1,\n",
    "    num_folds = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_if_false(x):\n",
    "    if not any(x.target.values):\n",
    "        print(x.session_id.values[0]) \n",
    "\n",
    "reranked_df.groupby('session_id').apply(\n",
    "    lambda x: print_if_false(x)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_df[(reranked_df.session_id == 1) & (reranked_df.target == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "\n",
    "item_features_df, train_sessions_df, train_purchases_df, test_sessions_df, candidate_items_df = get_dataframes()\n",
    "\n",
    "train_views_purch_df, val_purch_df = train_val_split(train_sessions_df, train_purchases_df,\n",
    "                                           n_sets=1,\n",
    "                                           ts_start='2021-05-01', ts_end='2021-06-01',\n",
    "                                           return_discarded=False)\n",
    "# create mapping\n",
    "item_mapping = create_mapping(item_features_df['item_id'])\n",
    "\n",
    "train_session_mapping = create_mapping(train_views_purch_df['session_id'])\n",
    "\n",
    "val_session_mapping = create_mapping(val_purch_df['session_id'])\n",
    "\n",
    "val_sessions_arr = get_mapped_sessions_to_recommend(\n",
    "    val_purch_df, val_session_mapping)\n",
    "\n",
    "candidates_val_ids = np.unique(val_purch_df['item_id'].values)\n",
    "items_to_ignore_val = get_items_to_exclude(item_features_df, candidates_val_ids)\n",
    "mapped_items_to_ignore_val = [item_mapping[elem] for elem in items_to_ignore_val]\n",
    "\n",
    "val_purch_df['session_id'] = val_purch_df['session_id'].map(val_session_mapping)\n",
    "val_purch_df['item_id'] = val_purch_df['item_id'].map(item_mapping)\n",
    "\n",
    "val_views_df = train_sessions_df[\n",
    "    (train_sessions_df.date >= '2021-05-01') & (train_sessions_df.date < '2021-06-01')][['session_id', 'item_id', 'date']]\n",
    "\n",
    "'''\n",
    "# create_URM does the mapping\n",
    "val_views_df['session_id'] = val_views_df['session_id'].map(val_session_mapping)\n",
    "val_views_df['item_id'] = val_views_df['item_id'].map(item_mapping)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_val_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_purch_df.sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_views_purch_df.sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "candidates_df_path = \"./Dataset/xgb_candidates/candidates_train_df.parquet\"\n",
    "\n",
    "models = []\n",
    "models_hyp = []\n",
    "\n",
    "if not os.path.exists(candidates_df_path):\n",
    "    # create URM_train\n",
    "    URM_train = create_URM(train_views_purch_df, train_session_mapping, item_mapping)\n",
    "    URM_val = create_URM(val_views_df, val_session_mapping, item_mapping)\n",
    "\n",
    "    # define pre optimized models and best hyperparameters\n",
    "    models.append(P3alphaRecommender(URM_train))\n",
    "    models_hyp.append(\n",
    "        {'topK': 479, 'alpha': 1.1764856470188576, 'normalize_similarity': True})\n",
    "\n",
    "    models.append(TopPop(URM_train))\n",
    "    models_hyp.append({})\n",
    "\n",
    "    models.append(ItemKNNCFRecommender(URM_train))\n",
    "    models_hyp.append(\n",
    "        {'shrink': 500, 'similarity': 'asymmetric', 'feature_weighting': 'none', 'topK': 495, 'normalize': True})\n",
    "\n",
    "    # fit models on URM_train\n",
    "    fit_models(models, models_hyp, mapped_items_to_ignore_val)\n",
    "\n",
    "    for model in models:\n",
    "        model.set_URM_train(URM_val)\n",
    "\n",
    "# generate candidates\n",
    "candidates_df = load_xgboost_train_df(session_ids=val_sessions_arr, val_purchases=val_purch_df, models=models,\n",
    "                                      cutoff=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add feature columns\n",
    "session_attributes_train_df, item_attributes_df = load_attributes(train_session_mapping=val_session_mapping,\n",
    "                                                                          item_mapping=item_mapping)\n",
    "                                                                          \n",
    "session_attributes_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidates_df = XGB_insert_item_feature(candidates_df, item_attributes_df)\n",
    "candidates_df = XGB_insert_session_feature(candidates_df, session_attributes_train_df)\n",
    "candidates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df[(candidates_df.session_id == 0) & (candidates_df.item_id == 9556)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = candidates_df[['target']]\n",
    "candidates_df = candidates_df.drop(columns='target')\n",
    "\n",
    "xgb_hyperparams, iteration = XGB_hypertune(\n",
    "    candidates_df=candidates_df, target_df=target_df, num_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGB_train(candidates_df=candidates_df, target_df=target_df, xgb_hyperparams=xgb_hyperparams,\n",
    "                      num_boost_round=iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = xgb_model.get_score()\n",
    "print(\"Feature importance: \" + str(feature_importance))\n",
    "\n",
    "# Remove useless columns and retrain\n",
    "cols_to_keep = list(feature_importance.keys())\n",
    "cols_to_keep.extend(['session_id', 'item_id'])\n",
    "\n",
    "to_drop = [col for col in candidates_df.columns.to_list() if col not in cols_to_keep]\n",
    "candidates_df = candidates_df.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGB_train(candidates_df=candidates_df, target_df=target_df, xgb_hyperparams=xgb_hyperparams,\n",
    "                      num_boost_round=iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_df = XGB_rerank(candidates_df=candidates_df, xgb_model=xgb_model, cutoff=100)\n",
    "reranked_df.to_parquet('./Dataset/xgb_candidates/reranked_df.parquet')\n",
    "reranked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_df = pd.read_parquet('./Dataset/xgb_candidates/reranked_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_df[reranked_df.session_id == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE SUBMISSION\n",
    "\n",
    "train_set_df = pd.concat([train_sessions_df, train_purchases_df])\n",
    "train_set_df.sort_values(by=['session_id', 'date'], inplace=True)\n",
    "train_set_df.reset_index(drop=True, inplace= True)\n",
    "\n",
    "train_session_mapping = create_mapping(train_set_df['session_id'])\n",
    "\n",
    "test_set_df = test_sessions_df\n",
    "test_session_mapping = create_mapping(test_set_df['session_id'])\n",
    "\n",
    "test_sessions_arr = get_mapped_sessions_to_recommend(test_set_df, test_session_mapping)\n",
    "\n",
    "candidates_val_ids = candidate_items_df['item_id'].values\n",
    "items_to_ignore = get_items_to_exclude(item_features_df, candidates_val_ids)\n",
    "mapped_items_to_ignore = [item_mapping[elem] for elem in items_to_ignore]\n",
    "\n",
    "candidates_df_path = \"./Dataset/xgb_candidates/candidates_test_df.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = []\n",
    "models_hyp = []\n",
    "\n",
    "if not os.path.exists(candidates_df_path):\n",
    "\n",
    "    URM_all = create_URM(train_set_df, train_session_mapping, item_mapping)\n",
    "    URM_test = create_URM(test_set_df, test_session_mapping, item_mapping)\n",
    "\n",
    "    models.append(P3alphaRecommender(URM_all))\n",
    "    models_hyp.append(\n",
    "        {'topK': 479, 'alpha': 1.1764856470188576, 'normalize_similarity': True})\n",
    "\n",
    "    models.append(TopPop(URM_all))\n",
    "    models_hyp.append({})\n",
    "\n",
    "    models.append(ItemKNNCFRecommender(URM_all))\n",
    "    models_hyp.append(\n",
    "        {'shrink': 500, 'similarity': 'asymmetric', 'feature_weighting': 'none', 'topK': 495, 'normalize': True})\n",
    "\n",
    "    fit_models(models, models_hyp, mapped_items_to_ignore)\n",
    "\n",
    "    for model in models:\n",
    "        model.set_URM_train(URM_test)\n",
    "        # WARNING: only works for models with an item-item similarity matrix\n",
    "\n",
    "sub_candidates_df = load_xgboost_test_df(session_ids=test_sessions_arr, models=models, cutoff=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_candidates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions with pretrained XGBoost model\n",
    "xgb_model = xgb.Booster()\n",
    "xgb_model.load_model(\"./Dataset/xgb_model/model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add feature columns\n",
    "session_attributes_test_df = load_attributes(test_session_mapping=test_session_mapping)\n",
    "session_attributes_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_candidates_df = XGB_insert_session_feature(\n",
    "    sub_candidates_df, session_attributes_test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_candidates_df = sub_candidates_df.drop(columns=['year', 'month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "to_drop = [col for col in sub_candidates_df.columns.to_list() if col not in cols_to_keep]\n",
    "\n",
    "sub_candidates_df = sub_candidates_df.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_candidates_df[(sub_candidates_df.session_id==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_df = XGB_rerank(candidates_df=sub_candidates_df[sub_candidates_df.session_id < 3], xgb_model=xgb_model, cutoff=100)\n",
    "reranked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission_XGB(reranked_df=reranked_df, item_mapping=item_mapping, session_mapping=test_session_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df_1 = candidates_df.copy()\n",
    "\n",
    "predictions = candidates_df_1.groupby('session_id').progress_apply(\n",
    "    lambda x: xgb_model.predict(\n",
    "        xgb.DMatrix(\n",
    "            x.drop(columns=['session_id', 'item_id']),\n",
    "            nthread=-1,\n",
    "            missing=np.NaN,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "scores = []\n",
    "for a in predictions.values:\n",
    "    scores.extend(a)\n",
    "\n",
    "# inverted because later I need to sort in ascending order for customer_id\n",
    "# candidates_df['score'] = [-a for a in scores]\n",
    "\n",
    "candidates_df_1['score'] = scores\n",
    "candidates_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df_1['session_id'] = candidates_df_1['session_id'].astype('int')\n",
    "\n",
    "reranked_df = candidates_df_1.sort_values(by=['session_id', 'score'], inplace=False, ascending=[True, False])\n",
    "reranked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_df[reranked_df.session_id == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_df = reranked_df.groupby('session_id').head(100)\n",
    "reranked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "662236ff4a3599a6369637d46054e6465a367b5e2f2cb09ee0460010b626acba"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('recsys')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}