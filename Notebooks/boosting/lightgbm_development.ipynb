{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LIGHTGBM DEVELOPMENT"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "candidates_df = pd.read_parquet('../../Dataset/xgb_candidates/candidates_train_df.parquet')\n",
    "target_df = pd.read_parquet('../../Dataset/xgb_candidates/target_train_df.parquet')\n",
    "true_candidates_df = pd.read_parquet('../../Dataset/xgb_candidates/true_candidates_train_df.parquet')\n",
    "candidates_test_df = pd.read_parquet('../../Dataset/xgb_candidates/candidates_test_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [session_id, item_id, EASE_R_Recommender_score, TopPopRecommender_score, GRU4RecRecommender_score, ItemKNN_CFCBF_HybridRecommender_score, UserKNNCFRecommenderStackedXGBoost_score, RecVAE_score, MultVAERecommender_score, SLIM_BPR_Recommender_score, RP3betaRecommender_score, target]\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>session_id</th>\n      <th>item_id</th>\n      <th>EASE_R_Recommender_score</th>\n      <th>TopPopRecommender_score</th>\n      <th>GRU4RecRecommender_score</th>\n      <th>ItemKNN_CFCBF_HybridRecommender_score</th>\n      <th>UserKNNCFRecommenderStackedXGBoost_score</th>\n      <th>RecVAE_score</th>\n      <th>MultVAERecommender_score</th>\n      <th>SLIM_BPR_Recommender_score</th>\n      <th>RP3betaRecommender_score</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = candidates_df[candidates_df.GRU4RecRecommender_score==-np.inf]\n",
    "df1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(candidates_df.groupby('session_id').size().values >= 100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "77146"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_df.session_id.nunique()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "          session_id  item_id  EASE_R_Recommender_score  \\\n0                  0     4558                  0.030360   \n1                  0     9304                  0.027252   \n2                  0    12750                  0.019521   \n3                  0    14160                  0.018259   \n4                  0    15987                  0.018073   \n...              ...      ...                       ...   \n17662834       81617    12818                  0.002294   \n17662835       81617    22162                  0.003331   \n17662836       81617     8591                  0.002203   \n17662837       81617    17561                  0.001553   \n17662838       81617     7410                  0.003348   \n\n          TopPopRecommender_score  GRU4RecRecommender_score  \\\n0                          1337.0                  3.828084   \n1                           231.0                  1.534931   \n2                          1672.0                  1.650607   \n3                           372.0                  1.170663   \n4                          1430.0                  1.117084   \n...                           ...                       ...   \n17662834                    240.0                  0.571261   \n17662835                    406.0                  0.750908   \n17662836                    110.0                 -0.119803   \n17662837                    806.0                  1.755220   \n17662838                    445.0                  0.874387   \n\n          ItemKNN_CFCBF_HybridRecommender_score  \\\n0                                      0.127591   \n1                                      0.089264   \n2                                      0.096536   \n3                                      0.073900   \n4                                      0.100167   \n...                                         ...   \n17662834                               0.004322   \n17662835                               0.021190   \n17662836                               0.003776   \n17662837                               0.003353   \n17662838                               0.021172   \n\n          UserKNNCFRecommenderStackedXGBoost_score  RecVAE_score  \\\n0                                         0.011288      8.892570   \n1                                         0.028409      9.167728   \n2                                         0.012672      7.459480   \n3                                         0.023248      9.427066   \n4                                         0.008669      7.559222   \n...                                            ...           ...   \n17662834                                  0.001687      0.851903   \n17662835                                  0.001593      5.043132   \n17662836                                  0.001582      2.071284   \n17662837                                  0.001886      3.336047   \n17662838                                  0.002284      4.803851   \n\n          MultVAERecommender_score  SLIM_BPR_Recommender_score  \\\n0                        10.339500                    0.140090   \n1                        10.625834                    0.104276   \n2                        10.653467                    0.159134   \n3                        10.077281                    0.085507   \n4                         8.963936                    0.091431   \n...                            ...                         ...   \n17662834                  3.290182                    0.004234   \n17662835                  6.574146                    0.033428   \n17662836                  2.871642                    0.003082   \n17662837                  7.237653                    0.008595   \n17662838                  9.201264                    0.018843   \n\n          RP3betaRecommender_score  target  \n0                         0.068946     0.0  \n1                         0.114748     0.0  \n2                         0.039530     0.0  \n3                         0.061862     0.0  \n4                         0.083270     0.0  \n...                            ...     ...  \n17662834                  0.019769     NaN  \n17662835                  0.019178     NaN  \n17662836                  0.018254     NaN  \n17662837                  0.018244     NaN  \n17662838                  0.018105     NaN  \n\n[17662839 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>session_id</th>\n      <th>item_id</th>\n      <th>EASE_R_Recommender_score</th>\n      <th>TopPopRecommender_score</th>\n      <th>GRU4RecRecommender_score</th>\n      <th>ItemKNN_CFCBF_HybridRecommender_score</th>\n      <th>UserKNNCFRecommenderStackedXGBoost_score</th>\n      <th>RecVAE_score</th>\n      <th>MultVAERecommender_score</th>\n      <th>SLIM_BPR_Recommender_score</th>\n      <th>RP3betaRecommender_score</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>4558</td>\n      <td>0.030360</td>\n      <td>1337.0</td>\n      <td>3.828084</td>\n      <td>0.127591</td>\n      <td>0.011288</td>\n      <td>8.892570</td>\n      <td>10.339500</td>\n      <td>0.140090</td>\n      <td>0.068946</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>9304</td>\n      <td>0.027252</td>\n      <td>231.0</td>\n      <td>1.534931</td>\n      <td>0.089264</td>\n      <td>0.028409</td>\n      <td>9.167728</td>\n      <td>10.625834</td>\n      <td>0.104276</td>\n      <td>0.114748</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>12750</td>\n      <td>0.019521</td>\n      <td>1672.0</td>\n      <td>1.650607</td>\n      <td>0.096536</td>\n      <td>0.012672</td>\n      <td>7.459480</td>\n      <td>10.653467</td>\n      <td>0.159134</td>\n      <td>0.039530</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>14160</td>\n      <td>0.018259</td>\n      <td>372.0</td>\n      <td>1.170663</td>\n      <td>0.073900</td>\n      <td>0.023248</td>\n      <td>9.427066</td>\n      <td>10.077281</td>\n      <td>0.085507</td>\n      <td>0.061862</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>15987</td>\n      <td>0.018073</td>\n      <td>1430.0</td>\n      <td>1.117084</td>\n      <td>0.100167</td>\n      <td>0.008669</td>\n      <td>7.559222</td>\n      <td>8.963936</td>\n      <td>0.091431</td>\n      <td>0.083270</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>17662834</th>\n      <td>81617</td>\n      <td>12818</td>\n      <td>0.002294</td>\n      <td>240.0</td>\n      <td>0.571261</td>\n      <td>0.004322</td>\n      <td>0.001687</td>\n      <td>0.851903</td>\n      <td>3.290182</td>\n      <td>0.004234</td>\n      <td>0.019769</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>17662835</th>\n      <td>81617</td>\n      <td>22162</td>\n      <td>0.003331</td>\n      <td>406.0</td>\n      <td>0.750908</td>\n      <td>0.021190</td>\n      <td>0.001593</td>\n      <td>5.043132</td>\n      <td>6.574146</td>\n      <td>0.033428</td>\n      <td>0.019178</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>17662836</th>\n      <td>81617</td>\n      <td>8591</td>\n      <td>0.002203</td>\n      <td>110.0</td>\n      <td>-0.119803</td>\n      <td>0.003776</td>\n      <td>0.001582</td>\n      <td>2.071284</td>\n      <td>2.871642</td>\n      <td>0.003082</td>\n      <td>0.018254</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>17662837</th>\n      <td>81617</td>\n      <td>17561</td>\n      <td>0.001553</td>\n      <td>806.0</td>\n      <td>1.755220</td>\n      <td>0.003353</td>\n      <td>0.001886</td>\n      <td>3.336047</td>\n      <td>7.237653</td>\n      <td>0.008595</td>\n      <td>0.018244</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>17662838</th>\n      <td>81617</td>\n      <td>7410</td>\n      <td>0.003348</td>\n      <td>445.0</td>\n      <td>0.874387</td>\n      <td>0.021172</td>\n      <td>0.002284</td>\n      <td>4.803851</td>\n      <td>9.201264</td>\n      <td>0.018843</td>\n      <td>0.018105</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>17662839 rows × 12 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_df['target'] = target_df['target']\n",
    "candidates_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "          session_id  item_id  EASE_R_Recommender_score  \\\n667                2     7444                       0.0   \n2727              13     8353                       0.0   \n2937              14    15650                       0.0   \n7263              35    16461                       0.0   \n7696              37    13844                       0.0   \n...              ...      ...                       ...   \n15455901       81556    15170                       0.0   \n15456887       81561      491                       0.0   \n15458338       81570     5971                       0.0   \n15459355       81576     9807                       0.0   \n15461371       81586     9087                       0.0   \n\n          TopPopRecommender_score  GRU4RecRecommender_score  \\\n667                           0.0                      -inf   \n2727                          0.0                      -inf   \n2937                          0.0                      -inf   \n7263                          0.0                      -inf   \n7696                          0.0                      -inf   \n...                           ...                       ...   \n15455901                      0.0                      -inf   \n15456887                      0.0                      -inf   \n15458338                      0.0                      -inf   \n15459355                      0.0                      -inf   \n15461371                      0.0                      -inf   \n\n          ItemKNN_CFCBF_HybridRecommender_score  \\\n667                                    0.001827   \n2727                                   0.003403   \n2937                                   0.004740   \n7263                                   0.005862   \n7696                                   0.000000   \n...                                         ...   \n15455901                               0.001417   \n15456887                               0.004553   \n15458338                               0.004982   \n15459355                               0.000286   \n15461371                               0.002675   \n\n          UserKNNCFRecommenderStackedXGBoost_score  RecVAE_score  \\\n667                                            0.0     -6.466080   \n2727                                           0.0     -7.097207   \n2937                                           0.0     -8.590778   \n7263                                           0.0     -7.337447   \n7696                                           0.0     -9.187262   \n...                                            ...           ...   \n15455901                                       0.0     -6.292246   \n15456887                                       0.0     -5.864120   \n15458338                                       0.0     -8.674047   \n15459355                                       0.0    -11.387657   \n15461371                                       0.0     -9.226401   \n\n          MultVAERecommender_score  target  \n667                     -11.838508       1  \n2727                    -11.838567       1  \n2937                    -11.838781       1  \n7263                    -11.838506       1  \n7696                    -11.838907       1  \n...                            ...     ...  \n15455901                -11.838485       1  \n15456887                -11.838431       1  \n15458338                -11.838867       1  \n15459355                -11.838817       1  \n15461371                -11.838481       1  \n\n[8290 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>session_id</th>\n      <th>item_id</th>\n      <th>EASE_R_Recommender_score</th>\n      <th>TopPopRecommender_score</th>\n      <th>GRU4RecRecommender_score</th>\n      <th>ItemKNN_CFCBF_HybridRecommender_score</th>\n      <th>UserKNNCFRecommenderStackedXGBoost_score</th>\n      <th>RecVAE_score</th>\n      <th>MultVAERecommender_score</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>667</th>\n      <td>2</td>\n      <td>7444</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.001827</td>\n      <td>0.0</td>\n      <td>-6.466080</td>\n      <td>-11.838508</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2727</th>\n      <td>13</td>\n      <td>8353</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.003403</td>\n      <td>0.0</td>\n      <td>-7.097207</td>\n      <td>-11.838567</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2937</th>\n      <td>14</td>\n      <td>15650</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.004740</td>\n      <td>0.0</td>\n      <td>-8.590778</td>\n      <td>-11.838781</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7263</th>\n      <td>35</td>\n      <td>16461</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.005862</td>\n      <td>0.0</td>\n      <td>-7.337447</td>\n      <td>-11.838506</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7696</th>\n      <td>37</td>\n      <td>13844</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>-9.187262</td>\n      <td>-11.838907</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15455901</th>\n      <td>81556</td>\n      <td>15170</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.001417</td>\n      <td>0.0</td>\n      <td>-6.292246</td>\n      <td>-11.838485</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15456887</th>\n      <td>81561</td>\n      <td>491</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.004553</td>\n      <td>0.0</td>\n      <td>-5.864120</td>\n      <td>-11.838431</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15458338</th>\n      <td>81570</td>\n      <td>5971</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.004982</td>\n      <td>0.0</td>\n      <td>-8.674047</td>\n      <td>-11.838867</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15459355</th>\n      <td>81576</td>\n      <td>9807</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.000286</td>\n      <td>0.0</td>\n      <td>-11.387657</td>\n      <td>-11.838817</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15461371</th>\n      <td>81586</td>\n      <td>9087</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.002675</td>\n      <td>0.0</td>\n      <td>-9.226401</td>\n      <td>-11.838481</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>8290 rows × 10 columns</p>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = candidates_df[(candidates_df.GRU4RecRecommender_score==-np.inf) & candidates_df.target==1]\n",
    "df1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "          session_id  item_id  EASE_R_Recommender_score  \\\n1025367         5368    16410                       0.0   \n1551009         8131      442                       0.0   \n1750340         9187    17341                       0.0   \n1985863        10413      348                       0.0   \n2187064        11482    17341                       0.0   \n2344110        12303    18465                       0.0   \n2738562        14377    23344                       0.0   \n2821603        14801     9153                       0.0   \n3367335        17698    21248                       0.0   \n4193386        22062     1752                       0.0   \n4298929        22631    17261                       0.0   \n4886767        25772    17126                       0.0   \n5171415        27289      348                       0.0   \n5210776        27490    13640                       0.0   \n5321464        28083    22808                       0.0   \n5779234        30519     3471                       0.0   \n5999033        31671     6533                       0.0   \n6005441        31703     9153                       0.0   \n6344897        33515     5778                       0.0   \n6407131        33839     9153                       0.0   \n6773330        35779    22692                       0.0   \n7140428        37709    15312                       0.0   \n7768402        41021     7776                       0.0   \n7937520        41912     5971                       0.0   \n8650421        45674    18059                       0.0   \n9053028        47801    16410                       0.0   \n9412317        49710    13473                       0.0   \n9773976        51639    14291                       0.0   \n10384314       54852      408                       0.0   \n11670242       61690      348                       0.0   \n11746936       62108    23344                       0.0   \n13039815       68940     5971                       0.0   \n13401534       70854    17900                       0.0   \n14082710       74439     7776                       0.0   \n14350237       75856     5971                       0.0   \n14649019       77435    15013                       0.0   \n14789539       78179     4761                       0.0   \n\n          TopPopRecommender_score  GRU4RecRecommender_score  \\\n1025367                       0.0                      -inf   \n1551009                       0.0                      -inf   \n1750340                       0.0                      -inf   \n1985863                       0.0                      -inf   \n2187064                       0.0                      -inf   \n2344110                       0.0                      -inf   \n2738562                       0.0                      -inf   \n2821603                       0.0                      -inf   \n3367335                       0.0                      -inf   \n4193386                       0.0                      -inf   \n4298929                       0.0                      -inf   \n4886767                       0.0                      -inf   \n5171415                       0.0                      -inf   \n5210776                       0.0                      -inf   \n5321464                       0.0                      -inf   \n5779234                       0.0                      -inf   \n5999033                       0.0                      -inf   \n6005441                       0.0                      -inf   \n6344897                       0.0                      -inf   \n6407131                       0.0                      -inf   \n6773330                       0.0                      -inf   \n7140428                       0.0                      -inf   \n7768402                       0.0                      -inf   \n7937520                       0.0                      -inf   \n8650421                       0.0                      -inf   \n9053028                       0.0                      -inf   \n9412317                       0.0                      -inf   \n9773976                       0.0                      -inf   \n10384314                      0.0                      -inf   \n11670242                      0.0                      -inf   \n11746936                      0.0                      -inf   \n13039815                      0.0                      -inf   \n13401534                      0.0                      -inf   \n14082710                      0.0                      -inf   \n14350237                      0.0                      -inf   \n14649019                      0.0                      -inf   \n14789539                      0.0                      -inf   \n\n          ItemKNN_CFCBF_HybridRecommender_score  \\\n1025367                                0.004279   \n1551009                                0.006661   \n1750340                                0.005455   \n1985863                                0.002980   \n2187064                                0.010244   \n2344110                                0.010727   \n2738562                                0.006597   \n2821603                                0.012047   \n3367335                                0.005882   \n4193386                                0.001106   \n4298929                                0.008717   \n4886767                                0.008024   \n5171415                                0.005142   \n5210776                                0.003996   \n5321464                                0.003875   \n5779234                                0.004191   \n5999033                                0.003144   \n6005441                                0.008998   \n6344897                                0.005528   \n6407131                                0.005587   \n6773330                                0.001425   \n7140428                                0.003933   \n7768402                                0.005791   \n7937520                                0.005851   \n8650421                                0.002707   \n9053028                                0.003581   \n9412317                                0.002351   \n9773976                                0.002282   \n10384314                               0.003502   \n11670242                               0.006943   \n11746936                               0.004912   \n13039815                               0.005851   \n13401534                               0.006165   \n14082710                               0.006268   \n14350237                               0.005851   \n14649019                               0.003005   \n14789539                               0.010007   \n\n          UserKNNCFRecommenderStackedXGBoost_score  RecVAE_score  \\\n1025367                                        0.0     -5.831815   \n1551009                                        0.0     -7.836661   \n1750340                                        0.0     -8.565071   \n1985863                                        0.0     -7.254121   \n2187064                                        0.0     -6.742564   \n2344110                                        0.0     -5.381531   \n2738562                                        0.0     -8.123440   \n2821603                                        0.0     -7.839392   \n3367335                                        0.0     -8.535583   \n4193386                                        0.0     -6.413959   \n4298929                                        0.0     -9.064165   \n4886767                                        0.0     -5.826672   \n5171415                                        0.0     -8.892335   \n5210776                                        0.0     -7.773003   \n5321464                                        0.0     -8.306108   \n5779234                                        0.0     -8.285899   \n5999033                                        0.0     -7.536201   \n6005441                                        0.0     -7.654274   \n6344897                                        0.0     -9.293538   \n6407131                                        0.0     -7.991458   \n6773330                                        0.0     -7.230810   \n7140428                                        0.0     -8.725203   \n7768402                                        0.0    -10.190992   \n7937520                                        0.0     -8.382350   \n8650421                                        0.0     -7.905792   \n9053028                                        0.0     -7.256852   \n9412317                                        0.0     -5.926598   \n9773976                                        0.0     -7.907065   \n10384314                                       0.0     -7.412572   \n11670242                                       0.0     -9.091652   \n11746936                                       0.0     -8.557734   \n13039815                                       0.0     -8.382350   \n13401534                                       0.0     -8.913552   \n14082710                                       0.0     -6.654979   \n14350237                                       0.0     -8.382349   \n14649019                                       0.0     -8.197910   \n14789539                                       0.0     -9.269692   \n\n          MultVAERecommender_score  target  is_fake  \n1025367                 -11.838864       1    False  \n1551009                 -11.839182       1    False  \n1750340                 -11.839021       1    False  \n1985863                 -11.839005       1    False  \n2187064                 -11.838596       1    False  \n2344110                 -11.839054       1    False  \n2738562                 -11.838804       1    False  \n2821603                 -11.838601       1    False  \n3367335                 -11.838984       1    False  \n4193386                 -11.838902       1    False  \n4298929                 -11.838704       1    False  \n4886767                 -11.838806       1    False  \n5171415                 -11.838939       1    False  \n5210776                 -11.838896       1    False  \n5321464                 -11.839029       1    False  \n5779234                 -11.838995       1    False  \n5999033                 -11.839058       1    False  \n6005441                 -11.838749       1    False  \n6344897                 -11.839055       1    False  \n6407131                 -11.839025       1    False  \n6773330                 -11.838969       1    False  \n7140428                 -11.839080       1    False  \n7768402                 -11.839078       1    False  \n7937520                 -11.838969       1    False  \n8650421                 -11.839156       1    False  \n9053028                 -11.839018       1    False  \n9412317                 -11.839085       1    False  \n9773976                 -11.839019       1    False  \n10384314                -11.839082       1    False  \n11670242                -11.838992       1    False  \n11746936                -11.838942       1    False  \n13039815                -11.838969       1    False  \n13401534                -11.839128       1    False  \n14082710                -11.838950       1    False  \n14350237                -11.838969       1    False  \n14649019                -11.839072       1    False  \n14789539                -11.839069       1    False  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>session_id</th>\n      <th>item_id</th>\n      <th>EASE_R_Recommender_score</th>\n      <th>TopPopRecommender_score</th>\n      <th>GRU4RecRecommender_score</th>\n      <th>ItemKNN_CFCBF_HybridRecommender_score</th>\n      <th>UserKNNCFRecommenderStackedXGBoost_score</th>\n      <th>RecVAE_score</th>\n      <th>MultVAERecommender_score</th>\n      <th>target</th>\n      <th>is_fake</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1025367</th>\n      <td>5368</td>\n      <td>16410</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.004279</td>\n      <td>0.0</td>\n      <td>-5.831815</td>\n      <td>-11.838864</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1551009</th>\n      <td>8131</td>\n      <td>442</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.006661</td>\n      <td>0.0</td>\n      <td>-7.836661</td>\n      <td>-11.839182</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1750340</th>\n      <td>9187</td>\n      <td>17341</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.005455</td>\n      <td>0.0</td>\n      <td>-8.565071</td>\n      <td>-11.839021</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1985863</th>\n      <td>10413</td>\n      <td>348</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.002980</td>\n      <td>0.0</td>\n      <td>-7.254121</td>\n      <td>-11.839005</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2187064</th>\n      <td>11482</td>\n      <td>17341</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.010244</td>\n      <td>0.0</td>\n      <td>-6.742564</td>\n      <td>-11.838596</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2344110</th>\n      <td>12303</td>\n      <td>18465</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.010727</td>\n      <td>0.0</td>\n      <td>-5.381531</td>\n      <td>-11.839054</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2738562</th>\n      <td>14377</td>\n      <td>23344</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.006597</td>\n      <td>0.0</td>\n      <td>-8.123440</td>\n      <td>-11.838804</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2821603</th>\n      <td>14801</td>\n      <td>9153</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.012047</td>\n      <td>0.0</td>\n      <td>-7.839392</td>\n      <td>-11.838601</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3367335</th>\n      <td>17698</td>\n      <td>21248</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.005882</td>\n      <td>0.0</td>\n      <td>-8.535583</td>\n      <td>-11.838984</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4193386</th>\n      <td>22062</td>\n      <td>1752</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.001106</td>\n      <td>0.0</td>\n      <td>-6.413959</td>\n      <td>-11.838902</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4298929</th>\n      <td>22631</td>\n      <td>17261</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.008717</td>\n      <td>0.0</td>\n      <td>-9.064165</td>\n      <td>-11.838704</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4886767</th>\n      <td>25772</td>\n      <td>17126</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.008024</td>\n      <td>0.0</td>\n      <td>-5.826672</td>\n      <td>-11.838806</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>5171415</th>\n      <td>27289</td>\n      <td>348</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.005142</td>\n      <td>0.0</td>\n      <td>-8.892335</td>\n      <td>-11.838939</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>5210776</th>\n      <td>27490</td>\n      <td>13640</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.003996</td>\n      <td>0.0</td>\n      <td>-7.773003</td>\n      <td>-11.838896</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>5321464</th>\n      <td>28083</td>\n      <td>22808</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.003875</td>\n      <td>0.0</td>\n      <td>-8.306108</td>\n      <td>-11.839029</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>5779234</th>\n      <td>30519</td>\n      <td>3471</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.004191</td>\n      <td>0.0</td>\n      <td>-8.285899</td>\n      <td>-11.838995</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>5999033</th>\n      <td>31671</td>\n      <td>6533</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.003144</td>\n      <td>0.0</td>\n      <td>-7.536201</td>\n      <td>-11.839058</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>6005441</th>\n      <td>31703</td>\n      <td>9153</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.008998</td>\n      <td>0.0</td>\n      <td>-7.654274</td>\n      <td>-11.838749</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>6344897</th>\n      <td>33515</td>\n      <td>5778</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.005528</td>\n      <td>0.0</td>\n      <td>-9.293538</td>\n      <td>-11.839055</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>6407131</th>\n      <td>33839</td>\n      <td>9153</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.005587</td>\n      <td>0.0</td>\n      <td>-7.991458</td>\n      <td>-11.839025</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>6773330</th>\n      <td>35779</td>\n      <td>22692</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.001425</td>\n      <td>0.0</td>\n      <td>-7.230810</td>\n      <td>-11.838969</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>7140428</th>\n      <td>37709</td>\n      <td>15312</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.003933</td>\n      <td>0.0</td>\n      <td>-8.725203</td>\n      <td>-11.839080</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>7768402</th>\n      <td>41021</td>\n      <td>7776</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.005791</td>\n      <td>0.0</td>\n      <td>-10.190992</td>\n      <td>-11.839078</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>7937520</th>\n      <td>41912</td>\n      <td>5971</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.005851</td>\n      <td>0.0</td>\n      <td>-8.382350</td>\n      <td>-11.838969</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>8650421</th>\n      <td>45674</td>\n      <td>18059</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.002707</td>\n      <td>0.0</td>\n      <td>-7.905792</td>\n      <td>-11.839156</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>9053028</th>\n      <td>47801</td>\n      <td>16410</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.003581</td>\n      <td>0.0</td>\n      <td>-7.256852</td>\n      <td>-11.839018</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>9412317</th>\n      <td>49710</td>\n      <td>13473</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.002351</td>\n      <td>0.0</td>\n      <td>-5.926598</td>\n      <td>-11.839085</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>9773976</th>\n      <td>51639</td>\n      <td>14291</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.002282</td>\n      <td>0.0</td>\n      <td>-7.907065</td>\n      <td>-11.839019</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>10384314</th>\n      <td>54852</td>\n      <td>408</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.003502</td>\n      <td>0.0</td>\n      <td>-7.412572</td>\n      <td>-11.839082</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>11670242</th>\n      <td>61690</td>\n      <td>348</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.006943</td>\n      <td>0.0</td>\n      <td>-9.091652</td>\n      <td>-11.838992</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>11746936</th>\n      <td>62108</td>\n      <td>23344</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.004912</td>\n      <td>0.0</td>\n      <td>-8.557734</td>\n      <td>-11.838942</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>13039815</th>\n      <td>68940</td>\n      <td>5971</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.005851</td>\n      <td>0.0</td>\n      <td>-8.382350</td>\n      <td>-11.838969</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>13401534</th>\n      <td>70854</td>\n      <td>17900</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.006165</td>\n      <td>0.0</td>\n      <td>-8.913552</td>\n      <td>-11.839128</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>14082710</th>\n      <td>74439</td>\n      <td>7776</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.006268</td>\n      <td>0.0</td>\n      <td>-6.654979</td>\n      <td>-11.838950</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>14350237</th>\n      <td>75856</td>\n      <td>5971</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.005851</td>\n      <td>0.0</td>\n      <td>-8.382349</td>\n      <td>-11.838969</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>14649019</th>\n      <td>77435</td>\n      <td>15013</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.003005</td>\n      <td>0.0</td>\n      <td>-8.197910</td>\n      <td>-11.839072</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>14789539</th>\n      <td>78179</td>\n      <td>4761</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-inf</td>\n      <td>0.010007</td>\n      <td>0.0</td>\n      <td>-9.269692</td>\n      <td>-11.839069</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_candidates_df[(true_candidates_df.target==1) & (true_candidates_df.is_fake==False) & (true_candidates_df.GRU4RecRecommender_score==-np.inf)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "          session_id  item_id  EASE_R_Recommender_score  \\\n22                 0    17813                  0.009132   \n912                4    21440                  0.003797   \n1055               5    12463                  0.019140   \n1251               6     9191                  0.017337   \n1545               7    13038                  0.001768   \n...              ...      ...                       ...   \n15435615       81609     6771                  0.018217   \n15436053       81611      973                  0.006346   \n15436559       81615    20251                  0.040063   \n15436778       81616    13864                  0.003650   \n15436970       81617     2445                  0.024162   \n\n          TopPopRecommender_score  GRU4RecRecommender_score  \\\n22                          323.0                  2.468104   \n912                        2234.0                  1.952044   \n1055                        143.0                  2.787538   \n1251                        459.0                  3.021174   \n1545                        281.0                  2.762492   \n...                           ...                       ...   \n15435615                    649.0                  2.429653   \n15436053                    183.0                  1.930094   \n15436559                    179.0                  1.222537   \n15436778                    311.0                  2.971658   \n15436970                   3432.0                  0.915228   \n\n          ItemKNN_CFCBF_HybridRecommender_score  \\\n22                                     0.051677   \n912                                    0.011432   \n1055                                   0.053186   \n1251                                   0.102537   \n1545                                   0.025756   \n...                                         ...   \n15435615                               0.091961   \n15436053                               0.060864   \n15436559                               0.118522   \n15436778                               0.016268   \n15436970                               0.061020   \n\n          UserKNNCFRecommenderStackedXGBoost_score  RecVAE_score  \\\n22                                        0.005836      6.686669   \n912                                       0.001013      6.178163   \n1055                                      0.019108     10.956239   \n1251                                      0.004520      9.438218   \n1545                                      0.000978      4.466962   \n...                                            ...           ...   \n15435615                                  0.016930      8.930421   \n15436053                                  0.002623      6.137790   \n15436559                                  0.011731      8.753224   \n15436778                                  0.009564      7.731315   \n15436970                                  0.014916      7.477494   \n\n          MultVAERecommender_score  target  is_fake  \n22                        4.895742       1    False  \n912                       4.548442       1    False  \n1055                      6.639421       1    False  \n1251                      7.195975       1    False  \n1545                      2.643122       1    False  \n...                            ...     ...      ...  \n15435615                  6.550053       1    False  \n15436053                  5.808233       1    False  \n15436559                  7.851810       1    False  \n15436778                  3.927487       1    False  \n15436970                  8.003838       1    False  \n\n[46960 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>session_id</th>\n      <th>item_id</th>\n      <th>EASE_R_Recommender_score</th>\n      <th>TopPopRecommender_score</th>\n      <th>GRU4RecRecommender_score</th>\n      <th>ItemKNN_CFCBF_HybridRecommender_score</th>\n      <th>UserKNNCFRecommenderStackedXGBoost_score</th>\n      <th>RecVAE_score</th>\n      <th>MultVAERecommender_score</th>\n      <th>target</th>\n      <th>is_fake</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>22</th>\n      <td>0</td>\n      <td>17813</td>\n      <td>0.009132</td>\n      <td>323.0</td>\n      <td>2.468104</td>\n      <td>0.051677</td>\n      <td>0.005836</td>\n      <td>6.686669</td>\n      <td>4.895742</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>912</th>\n      <td>4</td>\n      <td>21440</td>\n      <td>0.003797</td>\n      <td>2234.0</td>\n      <td>1.952044</td>\n      <td>0.011432</td>\n      <td>0.001013</td>\n      <td>6.178163</td>\n      <td>4.548442</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1055</th>\n      <td>5</td>\n      <td>12463</td>\n      <td>0.019140</td>\n      <td>143.0</td>\n      <td>2.787538</td>\n      <td>0.053186</td>\n      <td>0.019108</td>\n      <td>10.956239</td>\n      <td>6.639421</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1251</th>\n      <td>6</td>\n      <td>9191</td>\n      <td>0.017337</td>\n      <td>459.0</td>\n      <td>3.021174</td>\n      <td>0.102537</td>\n      <td>0.004520</td>\n      <td>9.438218</td>\n      <td>7.195975</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1545</th>\n      <td>7</td>\n      <td>13038</td>\n      <td>0.001768</td>\n      <td>281.0</td>\n      <td>2.762492</td>\n      <td>0.025756</td>\n      <td>0.000978</td>\n      <td>4.466962</td>\n      <td>2.643122</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15435615</th>\n      <td>81609</td>\n      <td>6771</td>\n      <td>0.018217</td>\n      <td>649.0</td>\n      <td>2.429653</td>\n      <td>0.091961</td>\n      <td>0.016930</td>\n      <td>8.930421</td>\n      <td>6.550053</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>15436053</th>\n      <td>81611</td>\n      <td>973</td>\n      <td>0.006346</td>\n      <td>183.0</td>\n      <td>1.930094</td>\n      <td>0.060864</td>\n      <td>0.002623</td>\n      <td>6.137790</td>\n      <td>5.808233</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>15436559</th>\n      <td>81615</td>\n      <td>20251</td>\n      <td>0.040063</td>\n      <td>179.0</td>\n      <td>1.222537</td>\n      <td>0.118522</td>\n      <td>0.011731</td>\n      <td>8.753224</td>\n      <td>7.851810</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>15436778</th>\n      <td>81616</td>\n      <td>13864</td>\n      <td>0.003650</td>\n      <td>311.0</td>\n      <td>2.971658</td>\n      <td>0.016268</td>\n      <td>0.009564</td>\n      <td>7.731315</td>\n      <td>3.927487</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>15436970</th>\n      <td>81617</td>\n      <td>2445</td>\n      <td>0.024162</td>\n      <td>3432.0</td>\n      <td>0.915228</td>\n      <td>0.061020</td>\n      <td>0.014916</td>\n      <td>7.477494</td>\n      <td>8.003838</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>46960 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_candidates_df[true_candidates_df.target==1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "            item_id  EASE_R_Recommender_score  TopPopRecommender_score  \\\nsession_id                                                               \n3               278                       278                      278   \n134             270                       270                      270   \n187             278                       278                      278   \n194             276                       276                      276   \n222             272                       272                      272   \n...             ...                       ...                      ...   \n49707           274                       274                      274   \n49773           276                       276                      276   \n49925           276                       276                      276   \n49962           277                       277                      277   \n49980           272                       272                      272   \n\n            GRU4RecRecommender_score  ItemKNN_CFCBF_HybridRecommender_score  \\\nsession_id                                                                    \n3                                278                                    278   \n134                              270                                    270   \n187                              278                                    278   \n194                              276                                    276   \n222                              272                                    272   \n...                              ...                                    ...   \n49707                            274                                    274   \n49773                            276                                    276   \n49925                            276                                    276   \n49962                            277                                    277   \n49980                            272                                    272   \n\n            UserKNNCFRecommenderStackedXGBoost_score  RecVAE_score  \\\nsession_id                                                           \n3                                                278           278   \n134                                              270           270   \n187                                              278           278   \n194                                              276           276   \n222                                              272           272   \n...                                              ...           ...   \n49707                                            274           274   \n49773                                            276           276   \n49925                                            276           276   \n49962                                            277           277   \n49980                                            272           272   \n\n            MultVAERecommender_score  \nsession_id                            \n3                                278  \n134                              270  \n187                              278  \n194                              276  \n222                              272  \n...                              ...  \n49707                            274  \n49773                            276  \n49925                            276  \n49962                            277  \n49980                            272  \n\n[705 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>item_id</th>\n      <th>EASE_R_Recommender_score</th>\n      <th>TopPopRecommender_score</th>\n      <th>GRU4RecRecommender_score</th>\n      <th>ItemKNN_CFCBF_HybridRecommender_score</th>\n      <th>UserKNNCFRecommenderStackedXGBoost_score</th>\n      <th>RecVAE_score</th>\n      <th>MultVAERecommender_score</th>\n    </tr>\n    <tr>\n      <th>session_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>278</td>\n      <td>278</td>\n      <td>278</td>\n      <td>278</td>\n      <td>278</td>\n      <td>278</td>\n      <td>278</td>\n      <td>278</td>\n    </tr>\n    <tr>\n      <th>134</th>\n      <td>270</td>\n      <td>270</td>\n      <td>270</td>\n      <td>270</td>\n      <td>270</td>\n      <td>270</td>\n      <td>270</td>\n      <td>270</td>\n    </tr>\n    <tr>\n      <th>187</th>\n      <td>278</td>\n      <td>278</td>\n      <td>278</td>\n      <td>278</td>\n      <td>278</td>\n      <td>278</td>\n      <td>278</td>\n      <td>278</td>\n    </tr>\n    <tr>\n      <th>194</th>\n      <td>276</td>\n      <td>276</td>\n      <td>276</td>\n      <td>276</td>\n      <td>276</td>\n      <td>276</td>\n      <td>276</td>\n      <td>276</td>\n    </tr>\n    <tr>\n      <th>222</th>\n      <td>272</td>\n      <td>272</td>\n      <td>272</td>\n      <td>272</td>\n      <td>272</td>\n      <td>272</td>\n      <td>272</td>\n      <td>272</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49707</th>\n      <td>274</td>\n      <td>274</td>\n      <td>274</td>\n      <td>274</td>\n      <td>274</td>\n      <td>274</td>\n      <td>274</td>\n      <td>274</td>\n    </tr>\n    <tr>\n      <th>49773</th>\n      <td>276</td>\n      <td>276</td>\n      <td>276</td>\n      <td>276</td>\n      <td>276</td>\n      <td>276</td>\n      <td>276</td>\n      <td>276</td>\n    </tr>\n    <tr>\n      <th>49925</th>\n      <td>276</td>\n      <td>276</td>\n      <td>276</td>\n      <td>276</td>\n      <td>276</td>\n      <td>276</td>\n      <td>276</td>\n      <td>276</td>\n    </tr>\n    <tr>\n      <th>49962</th>\n      <td>277</td>\n      <td>277</td>\n      <td>277</td>\n      <td>277</td>\n      <td>277</td>\n      <td>277</td>\n      <td>277</td>\n      <td>277</td>\n    </tr>\n    <tr>\n      <th>49980</th>\n      <td>272</td>\n      <td>272</td>\n      <td>272</td>\n      <td>272</td>\n      <td>272</td>\n      <td>272</td>\n      <td>272</td>\n      <td>272</td>\n    </tr>\n  </tbody>\n</table>\n<p>705 rows × 8 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_test_df[candidates_test_df.GRU4RecRecommender_score==0].groupby('session_id').count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "         session_id  item_id  EASE_R_Recommender_score  \\\n2102189       10829     4739                       0.0   \n2102190       10829     4770                       0.0   \n2102191       10829    15563                       0.0   \n2102192       10829    15559                       0.0   \n2102193       10829     4761                       0.0   \n...             ...      ...                       ...   \n2102411       10829     1197                       0.0   \n2102412       10829     6264                       0.0   \n2102413       10829    12108                       0.0   \n2102414       10829     8014                       0.0   \n2102415       10829    23088                       0.0   \n\n         TopPopRecommender_score  GRU4RecRecommender_score  \\\n2102189                    948.0                       0.0   \n2102190                    110.0                       0.0   \n2102191                    253.0                       0.0   \n2102192                    180.0                       0.0   \n2102193                     72.0                       0.0   \n...                          ...                       ...   \n2102411                   1719.0                       0.0   \n2102412                   1497.0                       0.0   \n2102413                   1450.0                       0.0   \n2102414                   2578.0                       0.0   \n2102415                   2039.0                       0.0   \n\n         ItemKNN_CFCBF_HybridRecommender_score  \\\n2102189                                    0.0   \n2102190                                    0.0   \n2102191                                    0.0   \n2102192                                    0.0   \n2102193                                    0.0   \n...                                        ...   \n2102411                                    0.0   \n2102412                                    0.0   \n2102413                                    0.0   \n2102414                                    0.0   \n2102415                                    0.0   \n\n         UserKNNCFRecommenderStackedXGBoost_score  RecVAE_score  \\\n2102189                                       0.0      0.141480   \n2102190                                       0.0     -1.042718   \n2102191                                       0.0     -0.954514   \n2102192                                       0.0     -0.633280   \n2102193                                       0.0     -0.746902   \n...                                           ...           ...   \n2102411                                       0.0      1.573100   \n2102412                                       0.0      1.728577   \n2102413                                       0.0      2.710261   \n2102414                                       0.0      2.624949   \n2102415                                       0.0      2.891080   \n\n         MultVAERecommender_score  \n2102189                  0.447383  \n2102190                 -2.514242  \n2102191                 -1.401856  \n2102192                 -2.243388  \n2102193                 -2.337119  \n...                           ...  \n2102411                  2.092361  \n2102412                  2.067122  \n2102413                  2.033361  \n2102414                  2.032845  \n2102415                  2.001871  \n\n[227 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>session_id</th>\n      <th>item_id</th>\n      <th>EASE_R_Recommender_score</th>\n      <th>TopPopRecommender_score</th>\n      <th>GRU4RecRecommender_score</th>\n      <th>ItemKNN_CFCBF_HybridRecommender_score</th>\n      <th>UserKNNCFRecommenderStackedXGBoost_score</th>\n      <th>RecVAE_score</th>\n      <th>MultVAERecommender_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2102189</th>\n      <td>10829</td>\n      <td>4739</td>\n      <td>0.0</td>\n      <td>948.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.141480</td>\n      <td>0.447383</td>\n    </tr>\n    <tr>\n      <th>2102190</th>\n      <td>10829</td>\n      <td>4770</td>\n      <td>0.0</td>\n      <td>110.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-1.042718</td>\n      <td>-2.514242</td>\n    </tr>\n    <tr>\n      <th>2102191</th>\n      <td>10829</td>\n      <td>15563</td>\n      <td>0.0</td>\n      <td>253.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.954514</td>\n      <td>-1.401856</td>\n    </tr>\n    <tr>\n      <th>2102192</th>\n      <td>10829</td>\n      <td>15559</td>\n      <td>0.0</td>\n      <td>180.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.633280</td>\n      <td>-2.243388</td>\n    </tr>\n    <tr>\n      <th>2102193</th>\n      <td>10829</td>\n      <td>4761</td>\n      <td>0.0</td>\n      <td>72.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.746902</td>\n      <td>-2.337119</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2102411</th>\n      <td>10829</td>\n      <td>1197</td>\n      <td>0.0</td>\n      <td>1719.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.573100</td>\n      <td>2.092361</td>\n    </tr>\n    <tr>\n      <th>2102412</th>\n      <td>10829</td>\n      <td>6264</td>\n      <td>0.0</td>\n      <td>1497.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.728577</td>\n      <td>2.067122</td>\n    </tr>\n    <tr>\n      <th>2102413</th>\n      <td>10829</td>\n      <td>12108</td>\n      <td>0.0</td>\n      <td>1450.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.710261</td>\n      <td>2.033361</td>\n    </tr>\n    <tr>\n      <th>2102414</th>\n      <td>10829</td>\n      <td>8014</td>\n      <td>0.0</td>\n      <td>2578.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.624949</td>\n      <td>2.032845</td>\n    </tr>\n    <tr>\n      <th>2102415</th>\n      <td>10829</td>\n      <td>23088</td>\n      <td>0.0</td>\n      <td>2039.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.891080</td>\n      <td>2.001871</td>\n    </tr>\n  </tbody>\n</table>\n<p>227 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_test_df[candidates_test_df.session_id==10829]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "input_df = candidates_df.drop(columns=['session_id', 'item_id'])\n",
    "train_data = lgb.Dataset(\n",
    "    input_df,\n",
    "    label=target_df,\n",
    "    group=candidates_df.groupby('session_id').size().to_list(),\n",
    "    feature_name=input_df.columns.to_list(),\n",
    "    categorical_feature = None\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'task': 'train', # 'predict'\n",
    "    'objective': 'rank_xendcg', # 'lambdarank'\n",
    "    'boosting': 'gbdt', # 'dart', 'goss'\n",
    "    # 'num_iterations': 1000,\n",
    "    'learning_rate': 1e-2,\n",
    "    'num_leaves': 50, # max 131072\n",
    "    'num_threads': 8,\n",
    "    'device_type': 'cpu', # smaller max_bin\n",
    "    'max_depth': 8, # -1 unlimited\n",
    "    'min_data_in_leaf': 20, # >= 0\n",
    "    'min_sum_hessian_in_leaf': 1e-3, \n",
    "    'bagging_fraction': 0.5, # 0 and 1, 1 is disabled\n",
    "    'bagging_freq': 4,  # 0 is disabled\n",
    "    'feature_fraction': 0.8, # 1 is disabled,\n",
    "    'feature_fraction_bynode': 0.9, # 1 is disabled\n",
    "    'extra_trees': True,\n",
    "    'early_stopping_round': 100,\n",
    "    'max_delta_step': 0.1, # <1 is no constraint\n",
    "    'lambda_l1': 1e-6,\n",
    "    'lambda_l2': 1e-3,\n",
    "    # 'drop_rate': 0.2, # for dart\n",
    "    'min_data_per_group': 20, # minimal number of data per categorical group\n",
    "    'cat_l2': 10, # L2 regularization in categorical split\n",
    "    'cat_smooth': 10,\n",
    "    'max_cat_to_onehot': 8,\n",
    "    'metric': 'map',\n",
    "    'eval_at': 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total groups: 51431, total data: 10311596\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.337940 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1785\n",
      "[LightGBM] [Info] Number of data points in the train set: 10311596, number of used features: 7\n",
      "[LightGBM] [Info] Total groups: 25715, total data: 5155739\n",
      "[LightGBM] [Info] Total groups: 51431, total data: 10311596\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1785\n",
      "[LightGBM] [Info] Number of data points in the train set: 10311596, number of used features: 7\n",
      "[LightGBM] [Info] Total groups: 25715, total data: 5155739\n",
      "[LightGBM] [Info] Total groups: 51430, total data: 10311478\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.287021 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1785\n",
      "[LightGBM] [Info] Number of data points in the train set: 10311478, number of used features: 7\n",
      "[LightGBM] [Info] Total groups: 25716, total data: 5155857\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'map@100-mean': [0.3230842320258269,\n  0.3294271962739819,\n  0.33044003291739993,\n  0.3334947967517136,\n  0.33744466237747267,\n  0.3395553091478505,\n  0.3388549251367953,\n  0.33910486251768573,\n  0.3393075990711316,\n  0.34294153696497903],\n 'map@100-stdv': [0.0018655086721626184,\n  0.008979705249162105,\n  0.006516079584304525,\n  0.008202541773379356,\n  0.007369163201076398,\n  0.007513861788091255,\n  0.006885302917330103,\n  0.0051924201835452715,\n  0.005176784400512108,\n  0.0041483903169852385]}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evals = lgb.cv(\n",
    "    params=params,\n",
    "    train_set=train_data,\n",
    "    num_boost_round=10,\n",
    "    return_cvbooster=False,\n",
    "    nfold=3,\n",
    "    # callbacks=[lgb.early_stopping(stopping_rounds=100)]\n",
    ")\n",
    "evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HYPERTUNE] Best mean validation score: 0.34294153696497903\n",
      "[HYPERTUNE] Std of best validation score: 0.0041483903169852385\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scores = list(evals.values())[0]\n",
    "\n",
    "print('[HYPERTUNE] Best mean validation score: ' + str(scores[-1]))\n",
    "print('[HYPERTUNE] Std of best validation score: ' + str(list(evals.values())[1][-1]))\n",
    "\n",
    "print(scores.index(scores[-1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# XGBOOST PIPELINE TESTING"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "item_features_df = pd.read_csv('../../Dataset/item_features.csv')\n",
    "test_sessions_df = pd.read_csv('../../Dataset/test_leaderboard_sessions.csv')\n",
    "candidate_items_df = pd.read_csv('../../Dataset/candidate_items.csv')\n",
    "train_sessions_df = pd.read_csv('../../Dataset/train_sessions.csv')\n",
    "train_purchases_df = pd.read_csv('../../Dataset/train_purchases.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "session_attributes_train_df = pd.read_csv('../../Dataset/xgb_attributes/session_attributes_train.csv')\n",
    "session_attributes_test_df = pd.read_csv('../../Dataset/xgb_attributes/session_attributes_leaderboard.csv')\n",
    "item_attributes_df = pd.read_csv('../../Dataset/xgb_attributes/item_attributes.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "from RecSys_Course_AT_PoliMi.Pipeline.xgboost.xgboost_utils import XGB_tune_test, XGB_test_submission\n",
    "from RecSys_Course_AT_PoliMi.Recommenders.NonPersonalizedRecommender import TopPop\n",
    "from RecSys_Course_AT_PoliMi.Recommenders.KNN.ItemKNNCFRecommender import ItemKNNCFRecommender\n",
    "from RecSys_Course_AT_PoliMi.Recommenders.GraphBased.RP3betaRecommender import RP3betaRecommender\n",
    "\n",
    "# SETUP\n",
    "model_classes = [RP3betaRecommender, TopPop, ItemKNNCFRecommender]\n",
    "models_hyp = [\n",
    "    {'topK': 156, 'alpha': 0.9867577596015882, 'beta': 0.3006127016015112, 'normalize_similarity': True},\n",
    "    {},\n",
    "    {'topK': 4780, 'shrink': 3115, 'normalize': True, 'feature_weighting': 'TF-IDF'}\n",
    "]\n",
    "is_content_based = [False, False, False]\n",
    "cutoffs = []  # TODO add specific cutoff for each model\n",
    "\n",
    "model_dict = {'model_classes': model_classes, 'models_hyp': models_hyp, 'is_content_based': is_content_based}\n",
    "\n",
    "URM_dict = {'view_weight': 0.2}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "xgb_model, reranked_df = XGB_tune_test(\n",
    "    train_sessions_df=train_sessions_df,\n",
    "    train_purchases_df=train_purchases_df,\n",
    "    item_features_df=item_features_df,\n",
    "    session_attributes_train_df=session_attributes_train_df,\n",
    "    item_attributes_df=item_attributes_df,\n",
    "    model_dict=model_dict,\n",
    "    URM_dict=URM_dict,\n",
    "    num_trials=1,\n",
    "    num_folds=3,\n",
    "    num_boost_round=10\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "sub_reranked_df = XGB_test_submission(\n",
    "    train_sessions_df=train_sessions_df,\n",
    "    train_purchases_df=train_purchases_df,\n",
    "    item_features_df=item_features_df,\n",
    "    test_sessions_df=test_sessions_df,\n",
    "    candidate_items_df=candidate_items_df,\n",
    "    session_attributes_test_df=session_attributes_test_df,\n",
    "    item_attributes_df=item_attributes_df,\n",
    "    model_dict=model_dict,\n",
    "    URM_dict=URM_dict,\n",
    "    xgb_model=xgb_model,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CHECK SUBMISSION VALIDITY"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "sub_reranked_df = pd.read_csv('../../Dataset/sub/submission_2022_06_06_at_18_34_27.csv')\n",
    "\n",
    "# CHECK IF SUBMISSION_DF IS WELL-CONSTRUCTED\n",
    "print(all(np.sort(sub_reranked_df['session_id'].unique()) == np.sort(test_sessions_df['session_id'].unique())))\n",
    "print(all(sub_reranked_df.groupby(['session_id']).count()['item_id'].values == 100))\n",
    "print(all(np.isin(sub_reranked_df['item_id'].unique(), candidate_items_df['item_id'].unique())))\n",
    "print(all(sub_reranked_df.groupby(['session_id'])['item_id'].nunique() == 100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "data": {
      "text/plain": "         session_id  item_id  rank\n0                26    20028     1\n1                26      107     2\n2                26    12958     3\n3                26      551     4\n4                26    20236     5\n...             ...      ...   ...\n4999995     4439757    17750    96\n4999996     4439757    13479    97\n4999997     4439757    22352    98\n4999998     4439757    19992    99\n4999999     4439757    19525   100\n\n[5000000 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>session_id</th>\n      <th>item_id</th>\n      <th>rank</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>26</td>\n      <td>20028</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>26</td>\n      <td>107</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26</td>\n      <td>12958</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>26</td>\n      <td>551</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>26</td>\n      <td>20236</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4999995</th>\n      <td>4439757</td>\n      <td>17750</td>\n      <td>96</td>\n    </tr>\n    <tr>\n      <th>4999996</th>\n      <td>4439757</td>\n      <td>13479</td>\n      <td>97</td>\n    </tr>\n    <tr>\n      <th>4999997</th>\n      <td>4439757</td>\n      <td>22352</td>\n      <td>98</td>\n    </tr>\n    <tr>\n      <th>4999998</th>\n      <td>4439757</td>\n      <td>19992</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>4999999</th>\n      <td>4439757</td>\n      <td>19525</td>\n      <td>100</td>\n    </tr>\n  </tbody>\n</table>\n<p>5000000 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_reranked_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CREATE DICTIONARY FROM LISTS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "{'ciao': 0, 'sei': 10, 'top': 200, 'score': 30}"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_name = ['ciao', 'sei', 'top', 'score']\n",
    "feature_importance = [0, 10, 200, 30]\n",
    "\n",
    "zip_features = zip(feature_name, feature_importance)\n",
    "feature_dict = dict(zip_features)\n",
    "feature_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sei': 10, 'top': 200, 'score': 30}\n"
     ]
    }
   ],
   "source": [
    "del_keys= []\n",
    "for key, value in feature_dict.items():\n",
    "    if value == 0:\n",
    "        del_keys.append(key)\n",
    "for key in del_keys:\n",
    "    feature_dict.pop(key)\n",
    "print(feature_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TESTING MAPPING AND PIPELINE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "from RecSys_Course_AT_PoliMi.Pipeline.utils import create_mapping, merge_views_purchases, get_mapped_sessions_to_recommend,get_items_to_exclude"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "test_sessions_df_copy = test_sessions_df.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "train_sessions_df['score'] = 0.2\n",
    "train_purchases_df['score'] = 1\n",
    "test_sessions_df['score'] = 0.2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "train_set_df = merge_views_purchases(train_sessions_df, train_purchases_df)\n",
    "\n",
    "train_session_mapping = create_mapping(train_set_df['session_id'])\n",
    "item_mapping = create_mapping(item_features_df['item_id'])\n",
    "test_session_mapping = create_mapping(test_sessions_df['session_id'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "{3: 0,\n 13: 1,\n 18: 2,\n 19: 3,\n 24: 4,\n 28: 5,\n 31: 6,\n 36: 7,\n 42: 8,\n 44: 9,\n 48: 10,\n 49: 11,\n 52: 12,\n 75: 13,\n 77: 14,\n 107: 15,\n 108: 16,\n 113: 17,\n 115: 18,\n 119: 19,\n 124: 20,\n 127: 21,\n 140: 22,\n 151: 23,\n 153: 24,\n 154: 25,\n 156: 26,\n 170: 27,\n 171: 28,\n 181: 29,\n 182: 30,\n 184: 31,\n 186: 32,\n 188: 33,\n 195: 34,\n 207: 35,\n 208: 36,\n 226: 37,\n 227: 38,\n 232: 39,\n 235: 40,\n 237: 41,\n 247: 42,\n 248: 43,\n 261: 44,\n 266: 45,\n 289: 46,\n 302: 47,\n 306: 48,\n 315: 49,\n 318: 50,\n 324: 51,\n 325: 52,\n 332: 53,\n 334: 54,\n 337: 55,\n 339: 56,\n 340: 57,\n 349: 58,\n 351: 59,\n 352: 60,\n 359: 61,\n 376: 62,\n 378: 63,\n 380: 64,\n 384: 65,\n 388: 66,\n 390: 67,\n 412: 68,\n 415: 69,\n 418: 70,\n 419: 71,\n 422: 72,\n 428: 73,\n 429: 74,\n 431: 75,\n 443: 76,\n 449: 77,\n 450: 78,\n 453: 79,\n 454: 80,\n 457: 81,\n 459: 82,\n 461: 83,\n 464: 84,\n 467: 85,\n 475: 86,\n 477: 87,\n 480: 88,\n 482: 89,\n 486: 90,\n 487: 91,\n 493: 92,\n 494: 93,\n 496: 94,\n 499: 95,\n 507: 96,\n 508: 97,\n 509: 98,\n 512: 99,\n 523: 100,\n 526: 101,\n 527: 102,\n 534: 103,\n 540: 104,\n 544: 105,\n 553: 106,\n 556: 107,\n 561: 108,\n 564: 109,\n 574: 110,\n 579: 111,\n 592: 112,\n 599: 113,\n 601: 114,\n 603: 115,\n 606: 116,\n 609: 117,\n 631: 118,\n 632: 119,\n 633: 120,\n 642: 121,\n 647: 122,\n 654: 123,\n 656: 124,\n 661: 125,\n 671: 126,\n 677: 127,\n 681: 128,\n 685: 129,\n 687: 130,\n 688: 131,\n 691: 132,\n 696: 133,\n 697: 134,\n 712: 135,\n 717: 136,\n 720: 137,\n 722: 138,\n 730: 139,\n 746: 140,\n 755: 141,\n 756: 142,\n 762: 143,\n 767: 144,\n 778: 145,\n 781: 146,\n 782: 147,\n 795: 148,\n 799: 149,\n 802: 150,\n 806: 151,\n 808: 152,\n 809: 153,\n 811: 154,\n 813: 155,\n 815: 156,\n 817: 157,\n 824: 158,\n 829: 159,\n 832: 160,\n 837: 161,\n 842: 162,\n 843: 163,\n 844: 164,\n 846: 165,\n 851: 166,\n 857: 167,\n 864: 168,\n 865: 169,\n 866: 170,\n 884: 171,\n 890: 172,\n 894: 173,\n 896: 174,\n 900: 175,\n 901: 176,\n 904: 177,\n 907: 178,\n 910: 179,\n 915: 180,\n 916: 181,\n 919: 182,\n 922: 183,\n 923: 184,\n 925: 185,\n 926: 186,\n 927: 187,\n 928: 188,\n 930: 189,\n 931: 190,\n 934: 191,\n 937: 192,\n 941: 193,\n 945: 194,\n 954: 195,\n 955: 196,\n 957: 197,\n 958: 198,\n 962: 199,\n 964: 200,\n 979: 201,\n 981: 202,\n 982: 203,\n 989: 204,\n 990: 205,\n 993: 206,\n 995: 207,\n 997: 208,\n 1008: 209,\n 1022: 210,\n 1023: 211,\n 1028: 212,\n 1029: 213,\n 1031: 214,\n 1033: 215,\n 1036: 216,\n 1041: 217,\n 1042: 218,\n 1046: 219,\n 1050: 220,\n 1052: 221,\n 1055: 222,\n 1060: 223,\n 1061: 224,\n 1074: 225,\n 1076: 226,\n 1078: 227,\n 1080: 228,\n 1089: 229,\n 1091: 230,\n 1096: 231,\n 1109: 232,\n 1113: 233,\n 1119: 234,\n 1129: 235,\n 1130: 236,\n 1131: 237,\n 1132: 238,\n 1135: 239,\n 1141: 240,\n 1142: 241,\n 1143: 242,\n 1149: 243,\n 1152: 244,\n 1154: 245,\n 1155: 246,\n 1174: 247,\n 1175: 248,\n 1179: 249,\n 1188: 250,\n 1190: 251,\n 1191: 252,\n 1192: 253,\n 1197: 254,\n 1199: 255,\n 1205: 256,\n 1212: 257,\n 1214: 258,\n 1215: 259,\n 1219: 260,\n 1225: 261,\n 1230: 262,\n 1241: 263,\n 1244: 264,\n 1252: 265,\n 1259: 266,\n 1274: 267,\n 1286: 268,\n 1288: 269,\n 1297: 270,\n 1303: 271,\n 1309: 272,\n 1312: 273,\n 1320: 274,\n 1325: 275,\n 1327: 276,\n 1330: 277,\n 1332: 278,\n 1333: 279,\n 1335: 280,\n 1341: 281,\n 1344: 282,\n 1346: 283,\n 1348: 284,\n 1355: 285,\n 1356: 286,\n 1359: 287,\n 1373: 288,\n 1382: 289,\n 1383: 290,\n 1401: 291,\n 1402: 292,\n 1412: 293,\n 1417: 294,\n 1419: 295,\n 1430: 296,\n 1433: 297,\n 1434: 298,\n 1435: 299,\n 1440: 300,\n 1441: 301,\n 1443: 302,\n 1457: 303,\n 1463: 304,\n 1464: 305,\n 1488: 306,\n 1490: 307,\n 1492: 308,\n 1499: 309,\n 1500: 310,\n 1502: 311,\n 1504: 312,\n 1516: 313,\n 1521: 314,\n 1527: 315,\n 1531: 316,\n 1537: 317,\n 1543: 318,\n 1545: 319,\n 1546: 320,\n 1547: 321,\n 1551: 322,\n 1553: 323,\n 1555: 324,\n 1562: 325,\n 1563: 326,\n 1573: 327,\n 1578: 328,\n 1581: 329,\n 1600: 330,\n 1613: 331,\n 1615: 332,\n 1622: 333,\n 1624: 334,\n 1626: 335,\n 1628: 336,\n 1630: 337,\n 1647: 338,\n 1655: 339,\n 1668: 340,\n 1671: 341,\n 1672: 342,\n 1684: 343,\n 1695: 344,\n 1707: 345,\n 1721: 346,\n 1722: 347,\n 1723: 348,\n 1733: 349,\n 1740: 350,\n 1756: 351,\n 1757: 352,\n 1758: 353,\n 1760: 354,\n 1767: 355,\n 1770: 356,\n 1773: 357,\n 1778: 358,\n 1780: 359,\n 1783: 360,\n 1785: 361,\n 1794: 362,\n 1795: 363,\n 1796: 364,\n 1800: 365,\n 1809: 366,\n 1811: 367,\n 1816: 368,\n 1821: 369,\n 1829: 370,\n 1830: 371,\n 1832: 372,\n 1837: 373,\n 1842: 374,\n 1845: 375,\n 1849: 376,\n 1851: 377,\n 1854: 378,\n 1859: 379,\n 1863: 380,\n 1869: 381,\n 1872: 382,\n 1873: 383,\n 1876: 384,\n 1881: 385,\n 1884: 386,\n 1886: 387,\n 1889: 388,\n 1890: 389,\n 1892: 390,\n 1903: 391,\n 1907: 392,\n 1914: 393,\n 1918: 394,\n 1922: 395,\n 1924: 396,\n 1925: 397,\n 1933: 398,\n 1937: 399,\n 1939: 400,\n 1943: 401,\n 1945: 402,\n 1949: 403,\n 1951: 404,\n 1953: 405,\n 1957: 406,\n 1962: 407,\n 1964: 408,\n 1966: 409,\n 1973: 410,\n 1978: 411,\n 1981: 412,\n 1983: 413,\n 1986: 414,\n 1999: 415,\n 2010: 416,\n 2016: 417,\n 2023: 418,\n 2026: 419,\n 2027: 420,\n 2030: 421,\n 2031: 422,\n 2032: 423,\n 2036: 424,\n 2043: 425,\n 2048: 426,\n 2050: 427,\n 2062: 428,\n 2070: 429,\n 2071: 430,\n 2077: 431,\n 2080: 432,\n 2083: 433,\n 2086: 434,\n 2089: 435,\n 2096: 436,\n 2098: 437,\n 2099: 438,\n 2100: 439,\n 2104: 440,\n 2106: 441,\n 2108: 442,\n 2114: 443,\n 2115: 444,\n 2121: 445,\n 2123: 446,\n 2125: 447,\n 2126: 448,\n 2127: 449,\n 2133: 450,\n 2135: 451,\n 2145: 452,\n 2149: 453,\n 2150: 454,\n 2162: 455,\n 2164: 456,\n 2174: 457,\n 2179: 458,\n 2185: 459,\n 2206: 460,\n 2208: 461,\n 2211: 462,\n 2214: 463,\n 2216: 464,\n 2223: 465,\n 2226: 466,\n 2228: 467,\n 2231: 468,\n 2234: 469,\n 2238: 470,\n 2239: 471,\n 2242: 472,\n 2250: 473,\n 2251: 474,\n 2255: 475,\n 2260: 476,\n 2262: 477,\n 2279: 478,\n 2281: 479,\n 2284: 480,\n 2288: 481,\n 2301: 482,\n 2304: 483,\n 2306: 484,\n 2315: 485,\n 2317: 486,\n 2319: 487,\n 2327: 488,\n 2328: 489,\n 2329: 490,\n 2330: 491,\n 2331: 492,\n 2336: 493,\n 2340: 494,\n 2345: 495,\n 2349: 496,\n 2353: 497,\n 2354: 498,\n 2355: 499,\n 2362: 500,\n 2376: 501,\n 2384: 502,\n 2388: 503,\n 2391: 504,\n 2394: 505,\n 2408: 506,\n 2411: 507,\n 2413: 508,\n 2417: 509,\n 2421: 510,\n 2423: 511,\n 2429: 512,\n 2431: 513,\n 2434: 514,\n 2438: 515,\n 2439: 516,\n 2449: 517,\n 2450: 518,\n 2451: 519,\n 2454: 520,\n 2460: 521,\n 2461: 522,\n 2462: 523,\n 2469: 524,\n 2471: 525,\n 2478: 526,\n 2479: 527,\n 2480: 528,\n 2484: 529,\n 2485: 530,\n 2487: 531,\n 2488: 532,\n 2489: 533,\n 2493: 534,\n 2494: 535,\n 2495: 536,\n 2498: 537,\n 2499: 538,\n 2502: 539,\n 2506: 540,\n 2507: 541,\n 2510: 542,\n 2512: 543,\n 2515: 544,\n 2517: 545,\n 2524: 546,\n 2531: 547,\n 2533: 548,\n 2557: 549,\n 2560: 550,\n 2561: 551,\n 2566: 552,\n 2576: 553,\n 2579: 554,\n 2583: 555,\n 2586: 556,\n 2592: 557,\n 2601: 558,\n 2608: 559,\n 2616: 560,\n 2624: 561,\n 2630: 562,\n 2635: 563,\n 2636: 564,\n 2637: 565,\n 2653: 566,\n 2654: 567,\n 2655: 568,\n 2664: 569,\n 2674: 570,\n 2676: 571,\n 2677: 572,\n 2687: 573,\n 2691: 574,\n 2693: 575,\n 2696: 576,\n 2697: 577,\n 2700: 578,\n 2705: 579,\n 2706: 580,\n 2708: 581,\n 2720: 582,\n 2723: 583,\n 2730: 584,\n 2736: 585,\n 2737: 586,\n 2742: 587,\n 2743: 588,\n 2745: 589,\n 2749: 590,\n 2751: 591,\n 2752: 592,\n 2764: 593,\n 2765: 594,\n 2775: 595,\n 2801: 596,\n 2803: 597,\n 2805: 598,\n 2809: 599,\n 2820: 600,\n 2821: 601,\n 2823: 602,\n 2830: 603,\n 2833: 604,\n 2841: 605,\n 2843: 606,\n 2857: 607,\n 2860: 608,\n 2863: 609,\n 2869: 610,\n 2875: 611,\n 2882: 612,\n 2883: 613,\n 2892: 614,\n 2901: 615,\n 2904: 616,\n 2906: 617,\n 2909: 618,\n 2916: 619,\n 2917: 620,\n 2923: 621,\n 2925: 622,\n 2930: 623,\n 2934: 624,\n 2937: 625,\n 2938: 626,\n 2942: 627,\n 2946: 628,\n 2947: 629,\n 2950: 630,\n 2955: 631,\n 2957: 632,\n 2961: 633,\n 2964: 634,\n 2974: 635,\n 2976: 636,\n 2994: 637,\n 3007: 638,\n 3011: 639,\n 3012: 640,\n 3018: 641,\n 3026: 642,\n 3033: 643,\n 3034: 644,\n 3035: 645,\n 3036: 646,\n 3037: 647,\n 3038: 648,\n 3044: 649,\n 3046: 650,\n 3069: 651,\n 3072: 652,\n 3083: 653,\n 3084: 654,\n 3089: 655,\n 3092: 656,\n 3099: 657,\n 3102: 658,\n 3104: 659,\n 3105: 660,\n 3106: 661,\n 3109: 662,\n 3114: 663,\n 3118: 664,\n 3119: 665,\n 3120: 666,\n 3123: 667,\n 3130: 668,\n 3132: 669,\n 3133: 670,\n 3138: 671,\n 3139: 672,\n 3152: 673,\n 3155: 674,\n 3159: 675,\n 3163: 676,\n 3167: 677,\n 3169: 678,\n 3171: 679,\n 3174: 680,\n 3175: 681,\n 3176: 682,\n 3178: 683,\n 3179: 684,\n 3187: 685,\n 3188: 686,\n 3204: 687,\n 3210: 688,\n 3218: 689,\n 3220: 690,\n 3222: 691,\n 3228: 692,\n 3230: 693,\n 3232: 694,\n 3235: 695,\n 3239: 696,\n 3241: 697,\n 3242: 698,\n 3243: 699,\n 3248: 700,\n 3250: 701,\n 3251: 702,\n 3257: 703,\n 3263: 704,\n 3265: 705,\n 3267: 706,\n 3269: 707,\n 3271: 708,\n 3273: 709,\n 3274: 710,\n 3276: 711,\n 3280: 712,\n 3281: 713,\n 3283: 714,\n 3288: 715,\n 3290: 716,\n 3292: 717,\n 3294: 718,\n 3299: 719,\n 3304: 720,\n 3308: 721,\n 3310: 722,\n 3314: 723,\n 3316: 724,\n 3318: 725,\n 3322: 726,\n 3326: 727,\n 3328: 728,\n 3331: 729,\n 3334: 730,\n 3340: 731,\n 3341: 732,\n 3348: 733,\n 3353: 734,\n 3355: 735,\n 3358: 736,\n 3361: 737,\n 3362: 738,\n 3363: 739,\n 3379: 740,\n 3382: 741,\n 3389: 742,\n 3391: 743,\n 3392: 744,\n 3393: 745,\n 3399: 746,\n 3402: 747,\n 3406: 748,\n 3418: 749,\n 3437: 750,\n 3441: 751,\n 3443: 752,\n 3447: 753,\n 3453: 754,\n 3467: 755,\n 3474: 756,\n 3489: 757,\n 3490: 758,\n 3494: 759,\n 3509: 760,\n 3512: 761,\n 3518: 762,\n 3524: 763,\n 3526: 764,\n 3530: 765,\n 3531: 766,\n 3533: 767,\n 3537: 768,\n 3538: 769,\n 3544: 770,\n 3546: 771,\n 3553: 772,\n 3554: 773,\n 3555: 774,\n 3556: 775,\n 3557: 776,\n 3559: 777,\n 3565: 778,\n 3568: 779,\n 3569: 780,\n 3573: 781,\n 3584: 782,\n 3590: 783,\n 3591: 784,\n 3593: 785,\n 3598: 786,\n 3601: 787,\n 3609: 788,\n 3610: 789,\n 3611: 790,\n 3616: 791,\n 3619: 792,\n 3624: 793,\n 3625: 794,\n 3627: 795,\n 3647: 796,\n 3648: 797,\n 3665: 798,\n 3666: 799,\n 3669: 800,\n 3684: 801,\n 3686: 802,\n 3695: 803,\n 3696: 804,\n 3697: 805,\n 3699: 806,\n 3713: 807,\n 3719: 808,\n 3721: 809,\n 3723: 810,\n 3728: 811,\n 3744: 812,\n 3748: 813,\n 3751: 814,\n 3755: 815,\n 3756: 816,\n 3758: 817,\n 3760: 818,\n 3762: 819,\n 3764: 820,\n 3771: 821,\n 3773: 822,\n 3775: 823,\n 3779: 824,\n 3785: 825,\n 3786: 826,\n 3787: 827,\n 3789: 828,\n 3791: 829,\n 3793: 830,\n 3794: 831,\n 3797: 832,\n 3804: 833,\n 3809: 834,\n 3815: 835,\n 3820: 836,\n 3836: 837,\n 3842: 838,\n 3854: 839,\n 3858: 840,\n 3859: 841,\n 3863: 842,\n 3868: 843,\n 3876: 844,\n 3878: 845,\n 3884: 846,\n 3889: 847,\n 3891: 848,\n 3896: 849,\n 3899: 850,\n 3903: 851,\n 3906: 852,\n 3907: 853,\n 3908: 854,\n 3910: 855,\n 3915: 856,\n 3919: 857,\n 3920: 858,\n 3928: 859,\n 3930: 860,\n 3937: 861,\n 3945: 862,\n 3949: 863,\n 3951: 864,\n 3953: 865,\n 3958: 866,\n 3967: 867,\n 3973: 868,\n 3976: 869,\n 3987: 870,\n 3988: 871,\n 3989: 872,\n 3990: 873,\n 3999: 874,\n 4000: 875,\n 4004: 876,\n 4008: 877,\n 4010: 878,\n 4011: 879,\n 4012: 880,\n 4015: 881,\n 4021: 882,\n 4023: 883,\n 4028: 884,\n 4031: 885,\n 4033: 886,\n 4037: 887,\n 4046: 888,\n 4058: 889,\n 4064: 890,\n 4066: 891,\n 4085: 892,\n 4089: 893,\n 4095: 894,\n 4096: 895,\n 4105: 896,\n 4113: 897,\n 4119: 898,\n 4122: 899,\n 4123: 900,\n 4130: 901,\n 4131: 902,\n 4132: 903,\n 4134: 904,\n 4136: 905,\n 4143: 906,\n 4147: 907,\n 4150: 908,\n 4153: 909,\n 4155: 910,\n 4157: 911,\n 4162: 912,\n 4163: 913,\n 4165: 914,\n 4170: 915,\n 4172: 916,\n 4176: 917,\n 4178: 918,\n 4189: 919,\n 4193: 920,\n 4206: 921,\n 4207: 922,\n 4211: 923,\n 4212: 924,\n 4217: 925,\n 4220: 926,\n 4221: 927,\n 4223: 928,\n 4228: 929,\n 4237: 930,\n 4239: 931,\n 4251: 932,\n 4252: 933,\n 4268: 934,\n 4270: 935,\n 4275: 936,\n 4285: 937,\n 4287: 938,\n 4292: 939,\n 4301: 940,\n 4312: 941,\n 4314: 942,\n 4320: 943,\n 4328: 944,\n 4330: 945,\n 4335: 946,\n 4343: 947,\n 4345: 948,\n 4347: 949,\n 4349: 950,\n 4354: 951,\n 4355: 952,\n 4356: 953,\n 4357: 954,\n 4373: 955,\n 4375: 956,\n 4379: 957,\n 4384: 958,\n 4385: 959,\n 4387: 960,\n 4390: 961,\n 4393: 962,\n 4397: 963,\n 4400: 964,\n 4401: 965,\n 4405: 966,\n 4406: 967,\n 4419: 968,\n 4421: 969,\n 4423: 970,\n 4424: 971,\n 4438: 972,\n 4440: 973,\n 4441: 974,\n 4450: 975,\n 4452: 976,\n 4454: 977,\n 4457: 978,\n 4464: 979,\n 4466: 980,\n 4481: 981,\n 4485: 982,\n 4486: 983,\n 4490: 984,\n 4504: 985,\n 4513: 986,\n 4516: 987,\n 4518: 988,\n 4520: 989,\n 4528: 990,\n 4529: 991,\n 4530: 992,\n 4533: 993,\n 4539: 994,\n 4541: 995,\n 4542: 996,\n 4543: 997,\n 4546: 998,\n 4554: 999,\n ...}"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_session_mapping"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "{2: 0,\n 3: 1,\n 4: 2,\n 7: 3,\n 8: 4,\n 9: 5,\n 10: 6,\n 11: 7,\n 13: 8,\n 14: 9,\n 16: 10,\n 17: 11,\n 18: 12,\n 19: 13,\n 20: 14,\n 21: 15,\n 24: 16,\n 25: 17,\n 26: 18,\n 27: 19,\n 28: 20,\n 29: 21,\n 30: 22,\n 32: 23,\n 33: 24,\n 35: 25,\n 36: 26,\n 38: 27,\n 39: 28,\n 40: 29,\n 42: 30,\n 43: 31,\n 44: 32,\n 45: 33,\n 46: 34,\n 47: 35,\n 49: 36,\n 50: 37,\n 51: 38,\n 52: 39,\n 53: 40,\n 54: 41,\n 55: 42,\n 56: 43,\n 57: 44,\n 58: 45,\n 59: 46,\n 60: 47,\n 62: 48,\n 63: 49,\n 64: 50,\n 65: 51,\n 67: 52,\n 68: 53,\n 69: 54,\n 70: 55,\n 74: 56,\n 75: 57,\n 76: 58,\n 78: 59,\n 79: 60,\n 80: 61,\n 81: 62,\n 82: 63,\n 83: 64,\n 84: 65,\n 86: 66,\n 88: 67,\n 89: 68,\n 90: 69,\n 91: 70,\n 92: 71,\n 93: 72,\n 94: 73,\n 95: 74,\n 97: 75,\n 98: 76,\n 99: 77,\n 100: 78,\n 101: 79,\n 102: 80,\n 103: 81,\n 104: 82,\n 105: 83,\n 106: 84,\n 107: 85,\n 108: 86,\n 109: 87,\n 111: 88,\n 112: 89,\n 113: 90,\n 114: 91,\n 115: 92,\n 116: 93,\n 117: 94,\n 118: 95,\n 119: 96,\n 120: 97,\n 121: 98,\n 123: 99,\n 124: 100,\n 125: 101,\n 126: 102,\n 127: 103,\n 128: 104,\n 130: 105,\n 131: 106,\n 132: 107,\n 133: 108,\n 134: 109,\n 135: 110,\n 137: 111,\n 138: 112,\n 140: 113,\n 141: 114,\n 142: 115,\n 143: 116,\n 145: 117,\n 146: 118,\n 147: 119,\n 148: 120,\n 149: 121,\n 150: 122,\n 151: 123,\n 152: 124,\n 153: 125,\n 154: 126,\n 155: 127,\n 156: 128,\n 157: 129,\n 158: 130,\n 160: 131,\n 161: 132,\n 162: 133,\n 163: 134,\n 164: 135,\n 165: 136,\n 166: 137,\n 167: 138,\n 168: 139,\n 169: 140,\n 170: 141,\n 171: 142,\n 172: 143,\n 173: 144,\n 174: 145,\n 175: 146,\n 176: 147,\n 177: 148,\n 178: 149,\n 179: 150,\n 180: 151,\n 181: 152,\n 182: 153,\n 183: 154,\n 184: 155,\n 185: 156,\n 187: 157,\n 188: 158,\n 189: 159,\n 190: 160,\n 191: 161,\n 192: 162,\n 193: 163,\n 194: 164,\n 196: 165,\n 197: 166,\n 198: 167,\n 201: 168,\n 202: 169,\n 203: 170,\n 204: 171,\n 205: 172,\n 206: 173,\n 207: 174,\n 208: 175,\n 209: 176,\n 211: 177,\n 214: 178,\n 215: 179,\n 217: 180,\n 218: 181,\n 219: 182,\n 220: 183,\n 221: 184,\n 222: 185,\n 223: 186,\n 224: 187,\n 225: 188,\n 227: 189,\n 228: 190,\n 230: 191,\n 231: 192,\n 233: 193,\n 234: 194,\n 236: 195,\n 238: 196,\n 239: 197,\n 240: 198,\n 241: 199,\n 242: 200,\n 243: 201,\n 244: 202,\n 245: 203,\n 246: 204,\n 247: 205,\n 248: 206,\n 251: 207,\n 252: 208,\n 253: 209,\n 257: 210,\n 258: 211,\n 259: 212,\n 260: 213,\n 262: 214,\n 263: 215,\n 264: 216,\n 265: 217,\n 266: 218,\n 267: 219,\n 269: 220,\n 271: 221,\n 273: 222,\n 274: 223,\n 275: 224,\n 277: 225,\n 279: 226,\n 280: 227,\n 281: 228,\n 282: 229,\n 283: 230,\n 284: 231,\n 285: 232,\n 286: 233,\n 287: 234,\n 289: 235,\n 290: 236,\n 291: 237,\n 292: 238,\n 293: 239,\n 294: 240,\n 295: 241,\n 296: 242,\n 297: 243,\n 298: 244,\n 299: 245,\n 300: 246,\n 301: 247,\n 303: 248,\n 305: 249,\n 307: 250,\n 308: 251,\n 309: 252,\n 310: 253,\n 311: 254,\n 312: 255,\n 313: 256,\n 314: 257,\n 315: 258,\n 317: 259,\n 318: 260,\n 319: 261,\n 320: 262,\n 321: 263,\n 322: 264,\n 323: 265,\n 324: 266,\n 325: 267,\n 326: 268,\n 327: 269,\n 328: 270,\n 329: 271,\n 330: 272,\n 331: 273,\n 332: 274,\n 333: 275,\n 334: 276,\n 335: 277,\n 336: 278,\n 337: 279,\n 339: 280,\n 340: 281,\n 342: 282,\n 343: 283,\n 344: 284,\n 345: 285,\n 346: 286,\n 347: 287,\n 348: 288,\n 350: 289,\n 351: 290,\n 352: 291,\n 353: 292,\n 354: 293,\n 357: 294,\n 358: 295,\n 359: 296,\n 360: 297,\n 363: 298,\n 364: 299,\n 366: 300,\n 367: 301,\n 368: 302,\n 369: 303,\n 370: 304,\n 371: 305,\n 372: 306,\n 373: 307,\n 374: 308,\n 375: 309,\n 376: 310,\n 378: 311,\n 380: 312,\n 381: 313,\n 382: 314,\n 384: 315,\n 385: 316,\n 386: 317,\n 388: 318,\n 389: 319,\n 390: 320,\n 391: 321,\n 392: 322,\n 393: 323,\n 394: 324,\n 396: 325,\n 397: 326,\n 398: 327,\n 399: 328,\n 400: 329,\n 401: 330,\n 402: 331,\n 403: 332,\n 404: 333,\n 405: 334,\n 406: 335,\n 408: 336,\n 409: 337,\n 410: 338,\n 412: 339,\n 413: 340,\n 416: 341,\n 417: 342,\n 418: 343,\n 419: 344,\n 420: 345,\n 421: 346,\n 422: 347,\n 423: 348,\n 424: 349,\n 425: 350,\n 426: 351,\n 427: 352,\n 428: 353,\n 429: 354,\n 430: 355,\n 432: 356,\n 433: 357,\n 434: 358,\n 435: 359,\n 436: 360,\n 437: 361,\n 438: 362,\n 439: 363,\n 441: 364,\n 442: 365,\n 445: 366,\n 446: 367,\n 447: 368,\n 448: 369,\n 450: 370,\n 451: 371,\n 452: 372,\n 455: 373,\n 456: 374,\n 457: 375,\n 458: 376,\n 459: 377,\n 460: 378,\n 461: 379,\n 462: 380,\n 464: 381,\n 465: 382,\n 466: 383,\n 467: 384,\n 468: 385,\n 469: 386,\n 470: 387,\n 471: 388,\n 472: 389,\n 473: 390,\n 475: 391,\n 477: 392,\n 478: 393,\n 479: 394,\n 480: 395,\n 481: 396,\n 482: 397,\n 483: 398,\n 484: 399,\n 485: 400,\n 486: 401,\n 488: 402,\n 489: 403,\n 491: 404,\n 492: 405,\n 493: 406,\n 494: 407,\n 495: 408,\n 496: 409,\n 497: 410,\n 498: 411,\n 499: 412,\n 500: 413,\n 502: 414,\n 504: 415,\n 505: 416,\n 506: 417,\n 507: 418,\n 508: 419,\n 509: 420,\n 511: 421,\n 512: 422,\n 513: 423,\n 514: 424,\n 515: 425,\n 516: 426,\n 517: 427,\n 518: 428,\n 519: 429,\n 521: 430,\n 522: 431,\n 523: 432,\n 524: 433,\n 525: 434,\n 526: 435,\n 527: 436,\n 528: 437,\n 530: 438,\n 532: 439,\n 533: 440,\n 534: 441,\n 537: 442,\n 538: 443,\n 539: 444,\n 541: 445,\n 542: 446,\n 543: 447,\n 544: 448,\n 545: 449,\n 546: 450,\n 549: 451,\n 550: 452,\n 551: 453,\n 553: 454,\n 554: 455,\n 555: 456,\n 556: 457,\n 557: 458,\n 558: 459,\n 559: 460,\n 560: 461,\n 561: 462,\n 562: 463,\n 563: 464,\n 564: 465,\n 566: 466,\n 567: 467,\n 568: 468,\n 569: 469,\n 570: 470,\n 571: 471,\n 572: 472,\n 573: 473,\n 574: 474,\n 575: 475,\n 576: 476,\n 578: 477,\n 579: 478,\n 580: 479,\n 581: 480,\n 582: 481,\n 585: 482,\n 586: 483,\n 587: 484,\n 588: 485,\n 590: 486,\n 591: 487,\n 592: 488,\n 593: 489,\n 594: 490,\n 596: 491,\n 597: 492,\n 598: 493,\n 599: 494,\n 600: 495,\n 601: 496,\n 602: 497,\n 603: 498,\n 604: 499,\n 605: 500,\n 606: 501,\n 608: 502,\n 609: 503,\n 610: 504,\n 611: 505,\n 612: 506,\n 613: 507,\n 614: 508,\n 615: 509,\n 617: 510,\n 619: 511,\n 620: 512,\n 621: 513,\n 622: 514,\n 623: 515,\n 624: 516,\n 625: 517,\n 627: 518,\n 628: 519,\n 629: 520,\n 630: 521,\n 631: 522,\n 632: 523,\n 633: 524,\n 635: 525,\n 636: 526,\n 637: 527,\n 638: 528,\n 639: 529,\n 640: 530,\n 641: 531,\n 642: 532,\n 643: 533,\n 644: 534,\n 647: 535,\n 648: 536,\n 649: 537,\n 650: 538,\n 651: 539,\n 652: 540,\n 653: 541,\n 654: 542,\n 655: 543,\n 657: 544,\n 659: 545,\n 660: 546,\n 661: 547,\n 662: 548,\n 664: 549,\n 666: 550,\n 667: 551,\n 668: 552,\n 669: 553,\n 670: 554,\n 672: 555,\n 673: 556,\n 674: 557,\n 675: 558,\n 676: 559,\n 677: 560,\n 678: 561,\n 680: 562,\n 681: 563,\n 682: 564,\n 683: 565,\n 684: 566,\n 685: 567,\n 687: 568,\n 689: 569,\n 690: 570,\n 691: 571,\n 692: 572,\n 693: 573,\n 694: 574,\n 695: 575,\n 696: 576,\n 697: 577,\n 698: 578,\n 699: 579,\n 700: 580,\n 701: 581,\n 702: 582,\n 704: 583,\n 705: 584,\n 706: 585,\n 707: 586,\n 708: 587,\n 709: 588,\n 711: 589,\n 712: 590,\n 713: 591,\n 714: 592,\n 715: 593,\n 717: 594,\n 718: 595,\n 719: 596,\n 720: 597,\n 721: 598,\n 722: 599,\n 724: 600,\n 725: 601,\n 726: 602,\n 728: 603,\n 729: 604,\n 730: 605,\n 732: 606,\n 733: 607,\n 735: 608,\n 736: 609,\n 737: 610,\n 738: 611,\n 739: 612,\n 740: 613,\n 741: 614,\n 742: 615,\n 743: 616,\n 744: 617,\n 745: 618,\n 747: 619,\n 749: 620,\n 750: 621,\n 751: 622,\n 752: 623,\n 753: 624,\n 754: 625,\n 756: 626,\n 758: 627,\n 759: 628,\n 761: 629,\n 763: 630,\n 765: 631,\n 766: 632,\n 767: 633,\n 768: 634,\n 769: 635,\n 770: 636,\n 771: 637,\n 772: 638,\n 773: 639,\n 774: 640,\n 775: 641,\n 776: 642,\n 777: 643,\n 778: 644,\n 779: 645,\n 780: 646,\n 781: 647,\n 782: 648,\n 783: 649,\n 784: 650,\n 785: 651,\n 786: 652,\n 787: 653,\n 788: 654,\n 789: 655,\n 790: 656,\n 791: 657,\n 792: 658,\n 793: 659,\n 794: 660,\n 795: 661,\n 796: 662,\n 797: 663,\n 799: 664,\n 800: 665,\n 801: 666,\n 802: 667,\n 803: 668,\n 804: 669,\n 805: 670,\n 806: 671,\n 807: 672,\n 808: 673,\n 809: 674,\n 810: 675,\n 811: 676,\n 812: 677,\n 816: 678,\n 819: 679,\n 821: 680,\n 822: 681,\n 823: 682,\n 824: 683,\n 825: 684,\n 826: 685,\n 828: 686,\n 829: 687,\n 831: 688,\n 832: 689,\n 834: 690,\n 835: 691,\n 836: 692,\n 837: 693,\n 838: 694,\n 839: 695,\n 840: 696,\n 841: 697,\n 842: 698,\n 843: 699,\n 844: 700,\n 845: 701,\n 846: 702,\n 847: 703,\n 848: 704,\n 849: 705,\n 850: 706,\n 852: 707,\n 853: 708,\n 854: 709,\n 855: 710,\n 856: 711,\n 857: 712,\n 858: 713,\n 859: 714,\n 860: 715,\n 861: 716,\n 862: 717,\n 864: 718,\n 865: 719,\n 866: 720,\n 867: 721,\n 868: 722,\n 870: 723,\n 871: 724,\n 872: 725,\n 873: 726,\n 875: 727,\n 877: 728,\n 878: 729,\n 880: 730,\n 881: 731,\n 882: 732,\n 883: 733,\n 885: 734,\n 886: 735,\n 887: 736,\n 888: 737,\n 889: 738,\n 891: 739,\n 892: 740,\n 893: 741,\n 894: 742,\n 895: 743,\n 896: 744,\n 897: 745,\n 898: 746,\n 899: 747,\n 900: 748,\n 901: 749,\n 903: 750,\n 904: 751,\n 905: 752,\n 906: 753,\n 908: 754,\n 909: 755,\n 911: 756,\n 912: 757,\n 913: 758,\n 914: 759,\n 915: 760,\n 916: 761,\n 917: 762,\n 918: 763,\n 919: 764,\n 920: 765,\n 921: 766,\n 922: 767,\n 923: 768,\n 924: 769,\n 925: 770,\n 928: 771,\n 929: 772,\n 930: 773,\n 931: 774,\n 932: 775,\n 933: 776,\n 934: 777,\n 935: 778,\n 936: 779,\n 937: 780,\n 938: 781,\n 939: 782,\n 940: 783,\n 941: 784,\n 942: 785,\n 944: 786,\n 945: 787,\n 947: 788,\n 948: 789,\n 949: 790,\n 950: 791,\n 951: 792,\n 952: 793,\n 953: 794,\n 954: 795,\n 955: 796,\n 957: 797,\n 958: 798,\n 959: 799,\n 960: 800,\n 961: 801,\n 962: 802,\n 963: 803,\n 965: 804,\n 966: 805,\n 969: 806,\n 970: 807,\n 971: 808,\n 972: 809,\n 973: 810,\n 974: 811,\n 975: 812,\n 977: 813,\n 978: 814,\n 980: 815,\n 981: 816,\n 982: 817,\n 983: 818,\n 984: 819,\n 985: 820,\n 986: 821,\n 987: 822,\n 988: 823,\n 989: 824,\n 990: 825,\n 991: 826,\n 992: 827,\n 994: 828,\n 995: 829,\n 996: 830,\n 997: 831,\n 998: 832,\n 999: 833,\n 1003: 834,\n 1004: 835,\n 1005: 836,\n 1006: 837,\n 1007: 838,\n 1008: 839,\n 1009: 840,\n 1010: 841,\n 1011: 842,\n 1012: 843,\n 1013: 844,\n 1014: 845,\n 1015: 846,\n 1016: 847,\n 1017: 848,\n 1018: 849,\n 1019: 850,\n 1020: 851,\n 1021: 852,\n 1022: 853,\n 1023: 854,\n 1024: 855,\n 1025: 856,\n 1026: 857,\n 1027: 858,\n 1028: 859,\n 1029: 860,\n 1030: 861,\n 1031: 862,\n 1033: 863,\n 1034: 864,\n 1035: 865,\n 1036: 866,\n 1037: 867,\n 1038: 868,\n 1039: 869,\n 1042: 870,\n 1043: 871,\n 1044: 872,\n 1045: 873,\n 1046: 874,\n 1047: 875,\n 1048: 876,\n 1049: 877,\n 1050: 878,\n 1051: 879,\n 1057: 880,\n 1058: 881,\n 1059: 882,\n 1060: 883,\n 1064: 884,\n 1065: 885,\n 1066: 886,\n 1067: 887,\n 1068: 888,\n 1069: 889,\n 1071: 890,\n 1072: 891,\n 1073: 892,\n 1074: 893,\n 1075: 894,\n 1076: 895,\n 1077: 896,\n 1078: 897,\n 1079: 898,\n 1080: 899,\n 1081: 900,\n 1082: 901,\n 1084: 902,\n 1085: 903,\n 1086: 904,\n 1087: 905,\n 1088: 906,\n 1089: 907,\n 1090: 908,\n 1091: 909,\n 1092: 910,\n 1094: 911,\n 1096: 912,\n 1097: 913,\n 1098: 914,\n 1099: 915,\n 1100: 916,\n 1101: 917,\n 1102: 918,\n 1103: 919,\n 1104: 920,\n 1105: 921,\n 1106: 922,\n 1107: 923,\n 1108: 924,\n 1109: 925,\n 1110: 926,\n 1112: 927,\n 1114: 928,\n 1115: 929,\n 1117: 930,\n 1119: 931,\n 1121: 932,\n 1122: 933,\n 1124: 934,\n 1125: 935,\n 1126: 936,\n 1127: 937,\n 1128: 938,\n 1129: 939,\n 1131: 940,\n 1132: 941,\n 1134: 942,\n 1135: 943,\n 1136: 944,\n 1137: 945,\n 1138: 946,\n 1141: 947,\n 1142: 948,\n 1143: 949,\n 1144: 950,\n 1145: 951,\n 1146: 952,\n 1147: 953,\n 1148: 954,\n 1149: 955,\n 1150: 956,\n 1151: 957,\n 1152: 958,\n 1154: 959,\n 1155: 960,\n 1157: 961,\n 1159: 962,\n 1160: 963,\n 1161: 964,\n 1162: 965,\n 1163: 966,\n 1164: 967,\n 1165: 968,\n 1166: 969,\n 1167: 970,\n 1169: 971,\n 1171: 972,\n 1172: 973,\n 1173: 974,\n 1174: 975,\n 1175: 976,\n 1176: 977,\n 1177: 978,\n 1178: 979,\n 1179: 980,\n 1181: 981,\n 1182: 982,\n 1183: 983,\n 1184: 984,\n 1186: 985,\n 1187: 986,\n 1188: 987,\n 1189: 988,\n 1191: 989,\n 1192: 990,\n 1193: 991,\n 1194: 992,\n 1195: 993,\n 1196: 994,\n 1198: 995,\n 1199: 996,\n 1201: 997,\n 1202: 998,\n 1203: 999,\n ...}"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_mapping"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "{26: 0,\n 200: 1,\n 205: 2,\n 495: 3,\n 521: 4,\n 587: 5,\n 721: 6,\n 810: 7,\n 886: 8,\n 1178: 9,\n 1208: 10,\n 1267: 11,\n 1304: 12,\n 1549: 13,\n 1592: 14,\n 1658: 15,\n 1725: 16,\n 1749: 17,\n 1912: 18,\n 1994: 19,\n 1998: 20,\n 2044: 21,\n 2075: 22,\n 2298: 23,\n 2334: 24,\n 2410: 25,\n 2526: 26,\n 2548: 27,\n 2600: 28,\n 2665: 29,\n 2726: 30,\n 2757: 31,\n 2762: 32,\n 2804: 33,\n 2853: 34,\n 3247: 35,\n 3305: 36,\n 3436: 37,\n 3589: 38,\n 3629: 39,\n 3871: 40,\n 3875: 41,\n 4039: 42,\n 4069: 43,\n 4091: 44,\n 4205: 45,\n 4297: 46,\n 4505: 47,\n 4630: 48,\n 4668: 49,\n 4749: 50,\n 4777: 51,\n 4936: 52,\n 5014: 53,\n 5023: 54,\n 5212: 55,\n 5231: 56,\n 5279: 57,\n 5383: 58,\n 5400: 59,\n 5480: 60,\n 5581: 61,\n 5870: 62,\n 5927: 63,\n 5928: 64,\n 6033: 65,\n 6050: 66,\n 6070: 67,\n 6206: 68,\n 6298: 69,\n 6342: 70,\n 6346: 71,\n 6378: 72,\n 6411: 73,\n 6486: 74,\n 6487: 75,\n 6897: 76,\n 7071: 77,\n 7204: 78,\n 7317: 79,\n 7318: 80,\n 7323: 81,\n 7328: 82,\n 7390: 83,\n 7816: 84,\n 7834: 85,\n 8136: 86,\n 8237: 87,\n 8314: 88,\n 8360: 89,\n 8394: 90,\n 8430: 91,\n 8448: 92,\n 8468: 93,\n 8527: 94,\n 8567: 95,\n 8656: 96,\n 8686: 97,\n 8761: 98,\n 8774: 99,\n 8871: 100,\n 8974: 101,\n 9176: 102,\n 9209: 103,\n 9327: 104,\n 9458: 105,\n 9511: 106,\n 9516: 107,\n 9571: 108,\n 9667: 109,\n 9700: 110,\n 9820: 111,\n 9865: 112,\n 9986: 113,\n 10013: 114,\n 10084: 115,\n 10111: 116,\n 10222: 117,\n 10250: 118,\n 10260: 119,\n 10321: 120,\n 10443: 121,\n 10513: 122,\n 10517: 123,\n 10583: 124,\n 10663: 125,\n 10699: 126,\n 10713: 127,\n 10734: 128,\n 10819: 129,\n 11038: 130,\n 11152: 131,\n 11335: 132,\n 11380: 133,\n 11443: 134,\n 11534: 135,\n 11600: 136,\n 11814: 137,\n 11954: 138,\n 11984: 139,\n 11995: 140,\n 12031: 141,\n 12348: 142,\n 12402: 143,\n 12405: 144,\n 12633: 145,\n 12657: 146,\n 12695: 147,\n 12815: 148,\n 12988: 149,\n 13006: 150,\n 13158: 151,\n 13167: 152,\n 13343: 153,\n 13375: 154,\n 13388: 155,\n 13593: 156,\n 13706: 157,\n 13759: 158,\n 13918: 159,\n 13925: 160,\n 13962: 161,\n 14053: 162,\n 14073: 163,\n 14104: 164,\n 14175: 165,\n 14328: 166,\n 14456: 167,\n 14475: 168,\n 14831: 169,\n 14967: 170,\n 14976: 171,\n 15133: 172,\n 15251: 173,\n 15342: 174,\n 15394: 175,\n 15425: 176,\n 15453: 177,\n 15581: 178,\n 15825: 179,\n 16102: 180,\n 16137: 181,\n 16145: 182,\n 16193: 183,\n 16407: 184,\n 16426: 185,\n 16554: 186,\n 16681: 187,\n 17025: 188,\n 17073: 189,\n 17076: 190,\n 17122: 191,\n 17272: 192,\n 17464: 193,\n 17574: 194,\n 17930: 195,\n 18065: 196,\n 18124: 197,\n 18131: 198,\n 18132: 199,\n 18177: 200,\n 18224: 201,\n 18233: 202,\n 18530: 203,\n 18660: 204,\n 18706: 205,\n 18740: 206,\n 18783: 207,\n 18808: 208,\n 19140: 209,\n 19161: 210,\n 19242: 211,\n 19243: 212,\n 19299: 213,\n 19415: 214,\n 19594: 215,\n 19639: 216,\n 19675: 217,\n 19714: 218,\n 19800: 219,\n 19822: 220,\n 19841: 221,\n 19921: 222,\n 19974: 223,\n 20096: 224,\n 20146: 225,\n 20210: 226,\n 20415: 227,\n 20443: 228,\n 20475: 229,\n 20525: 230,\n 20731: 231,\n 20737: 232,\n 20809: 233,\n 20854: 234,\n 20883: 235,\n 20932: 236,\n 20963: 237,\n 21105: 238,\n 21174: 239,\n 21219: 240,\n 21503: 241,\n 21533: 242,\n 21575: 243,\n 21619: 244,\n 21656: 245,\n 21669: 246,\n 21678: 247,\n 21912: 248,\n 21986: 249,\n 22089: 250,\n 22290: 251,\n 22376: 252,\n 22456: 253,\n 22481: 254,\n 22950: 255,\n 22952: 256,\n 22962: 257,\n 22971: 258,\n 23165: 259,\n 23204: 260,\n 23206: 261,\n 23298: 262,\n 23341: 263,\n 23404: 264,\n 23494: 265,\n 23501: 266,\n 23590: 267,\n 23682: 268,\n 23768: 269,\n 23803: 270,\n 23804: 271,\n 23978: 272,\n 23979: 273,\n 23993: 274,\n 24062: 275,\n 24086: 276,\n 24135: 277,\n 24256: 278,\n 24344: 279,\n 24375: 280,\n 24470: 281,\n 24556: 282,\n 24578: 283,\n 24645: 284,\n 24646: 285,\n 24711: 286,\n 24936: 287,\n 25047: 288,\n 25175: 289,\n 25248: 290,\n 25252: 291,\n 25291: 292,\n 25353: 293,\n 25374: 294,\n 25457: 295,\n 25688: 296,\n 25699: 297,\n 25823: 298,\n 25830: 299,\n 25852: 300,\n 25952: 301,\n 26027: 302,\n 26102: 303,\n 26370: 304,\n 26450: 305,\n 26632: 306,\n 26717: 307,\n 26932: 308,\n 26940: 309,\n 27142: 310,\n 27216: 311,\n 27239: 312,\n 27247: 313,\n 27344: 314,\n 27382: 315,\n 27411: 316,\n 27424: 317,\n 27445: 318,\n 27499: 319,\n 27512: 320,\n 27541: 321,\n 27560: 322,\n 27586: 323,\n 27633: 324,\n 27693: 325,\n 27748: 326,\n 27859: 327,\n 28037: 328,\n 28121: 329,\n 28194: 330,\n 28709: 331,\n 28731: 332,\n 28732: 333,\n 28824: 334,\n 28891: 335,\n 28905: 336,\n 28935: 337,\n 28940: 338,\n 29070: 339,\n 29136: 340,\n 29189: 341,\n 29420: 342,\n 29455: 343,\n 29616: 344,\n 29703: 345,\n 29901: 346,\n 30023: 347,\n 30084: 348,\n 30230: 349,\n 30238: 350,\n 30269: 351,\n 30300: 352,\n 30339: 353,\n 30403: 354,\n 30465: 355,\n 30480: 356,\n 30486: 357,\n 30844: 358,\n 31027: 359,\n 31053: 360,\n 31386: 361,\n 31517: 362,\n 31527: 363,\n 31742: 364,\n 31818: 365,\n 31824: 366,\n 31877: 367,\n 31919: 368,\n 32057: 369,\n 32128: 370,\n 32194: 371,\n 32288: 372,\n 32328: 373,\n 32357: 374,\n 32537: 375,\n 32549: 376,\n 32683: 377,\n 32722: 378,\n 32825: 379,\n 32872: 380,\n 32914: 381,\n 33050: 382,\n 33168: 383,\n 33345: 384,\n 33403: 385,\n 33506: 386,\n 33616: 387,\n 33915: 388,\n 34013: 389,\n 34246: 390,\n 34316: 391,\n 34724: 392,\n 34980: 393,\n 35019: 394,\n 35078: 395,\n 35191: 396,\n 35219: 397,\n 35552: 398,\n 35580: 399,\n 35593: 400,\n 35603: 401,\n 35842: 402,\n 36045: 403,\n 36304: 404,\n 36307: 405,\n 36387: 406,\n 36459: 407,\n 36905: 408,\n 37274: 409,\n 37327: 410,\n 37369: 411,\n 37377: 412,\n 37417: 413,\n 37510: 414,\n 37610: 415,\n 37723: 416,\n 37746: 417,\n 37808: 418,\n 37836: 419,\n 37890: 420,\n 37902: 421,\n 38018: 422,\n 38065: 423,\n 38076: 424,\n 38146: 425,\n 38183: 426,\n 38290: 427,\n 38386: 428,\n 38415: 429,\n 38448: 430,\n 38464: 431,\n 38497: 432,\n 38507: 433,\n 38556: 434,\n 38573: 435,\n 38660: 436,\n 38718: 437,\n 38940: 438,\n 39023: 439,\n 39175: 440,\n 39176: 441,\n 39177: 442,\n 39269: 443,\n 39359: 444,\n 39394: 445,\n 39403: 446,\n 39452: 447,\n 39510: 448,\n 39655: 449,\n 39657: 450,\n 39707: 451,\n 40213: 452,\n 40322: 453,\n 40337: 454,\n 40430: 455,\n 40511: 456,\n 40544: 457,\n 40606: 458,\n 40792: 459,\n 40868: 460,\n 40910: 461,\n 41087: 462,\n 41133: 463,\n 41208: 464,\n 41214: 465,\n 41242: 466,\n 41461: 467,\n 41471: 468,\n 41518: 469,\n 41522: 470,\n 41625: 471,\n 42057: 472,\n 42214: 473,\n 42218: 474,\n 42310: 475,\n 42328: 476,\n 42346: 477,\n 42390: 478,\n 42576: 479,\n 42681: 480,\n 42762: 481,\n 42766: 482,\n 42841: 483,\n 43051: 484,\n 43110: 485,\n 43120: 486,\n 43131: 487,\n 43142: 488,\n 43309: 489,\n 43320: 490,\n 43347: 491,\n 43408: 492,\n 43504: 493,\n 43505: 494,\n 43600: 495,\n 43611: 496,\n 43738: 497,\n 43744: 498,\n 43761: 499,\n 43803: 500,\n 43805: 501,\n 43808: 502,\n 43907: 503,\n 43968: 504,\n 44004: 505,\n 44214: 506,\n 44255: 507,\n 44531: 508,\n 44574: 509,\n 44690: 510,\n 44751: 511,\n 44820: 512,\n 44869: 513,\n 44894: 514,\n 44951: 515,\n 45028: 516,\n 45072: 517,\n 45108: 518,\n 45111: 519,\n 45166: 520,\n 45225: 521,\n 45302: 522,\n 45315: 523,\n 45398: 524,\n 45403: 525,\n 45413: 526,\n 45706: 527,\n 45763: 528,\n 45825: 529,\n 45828: 530,\n 45851: 531,\n 45967: 532,\n 46042: 533,\n 46208: 534,\n 46311: 535,\n 46315: 536,\n 46322: 537,\n 46476: 538,\n 46784: 539,\n 47042: 540,\n 47150: 541,\n 47167: 542,\n 47254: 543,\n 47311: 544,\n 47341: 545,\n 47348: 546,\n 47405: 547,\n 47632: 548,\n 47649: 549,\n 47658: 550,\n 47680: 551,\n 47695: 552,\n 47740: 553,\n 47833: 554,\n 48031: 555,\n 48055: 556,\n 48277: 557,\n 48442: 558,\n 48487: 559,\n 48592: 560,\n 48744: 561,\n 48786: 562,\n 48799: 563,\n 48889: 564,\n 49004: 565,\n 49029: 566,\n 49158: 567,\n 49174: 568,\n 49264: 569,\n 49336: 570,\n 49399: 571,\n 49435: 572,\n 49514: 573,\n 49662: 574,\n 49902: 575,\n 50011: 576,\n 50154: 577,\n 50168: 578,\n 50366: 579,\n 50425: 580,\n 50440: 581,\n 50447: 582,\n 50473: 583,\n 50697: 584,\n 50765: 585,\n 50912: 586,\n 50981: 587,\n 51114: 588,\n 51447: 589,\n 51500: 590,\n 51753: 591,\n 51917: 592,\n 51956: 593,\n 52127: 594,\n 52162: 595,\n 52256: 596,\n 52353: 597,\n 52782: 598,\n 53004: 599,\n 53071: 600,\n 53105: 601,\n 53125: 602,\n 53320: 603,\n 53328: 604,\n 53361: 605,\n 53406: 606,\n 53496: 607,\n 53513: 608,\n 53537: 609,\n 53679: 610,\n 53704: 611,\n 53726: 612,\n 53755: 613,\n 53803: 614,\n 53977: 615,\n 54047: 616,\n 54142: 617,\n 54190: 618,\n 54206: 619,\n 54208: 620,\n 54277: 621,\n 54394: 622,\n 54442: 623,\n 54590: 624,\n 54841: 625,\n 54893: 626,\n 55018: 627,\n 55172: 628,\n 55190: 629,\n 55273: 630,\n 55583: 631,\n 55611: 632,\n 55658: 633,\n 55765: 634,\n 55777: 635,\n 55985: 636,\n 56137: 637,\n 56349: 638,\n 56473: 639,\n 56572: 640,\n 56686: 641,\n 56741: 642,\n 56757: 643,\n 56821: 644,\n 56886: 645,\n 56993: 646,\n 57055: 647,\n 57195: 648,\n 57222: 649,\n 57294: 650,\n 57438: 651,\n 57517: 652,\n 57576: 653,\n 57578: 654,\n 57772: 655,\n 57814: 656,\n 58143: 657,\n 58262: 658,\n 58335: 659,\n 58368: 660,\n 58378: 661,\n 58383: 662,\n 58403: 663,\n 58437: 664,\n 58615: 665,\n 58710: 666,\n 59113: 667,\n 59215: 668,\n 59241: 669,\n 59280: 670,\n 59305: 671,\n 59367: 672,\n 59381: 673,\n 59590: 674,\n 59623: 675,\n 59743: 676,\n 59855: 677,\n 59910: 678,\n 59997: 679,\n 59998: 680,\n 60075: 681,\n 60198: 682,\n 60297: 683,\n 60386: 684,\n 60495: 685,\n 60497: 686,\n 60518: 687,\n 60590: 688,\n 60599: 689,\n 60662: 690,\n 60664: 691,\n 60666: 692,\n 60713: 693,\n 60789: 694,\n 60864: 695,\n 60912: 696,\n 60994: 697,\n 61009: 698,\n 61275: 699,\n 61281: 700,\n 61537: 701,\n 61725: 702,\n 61727: 703,\n 61992: 704,\n 62303: 705,\n 62411: 706,\n 62533: 707,\n 62607: 708,\n 62700: 709,\n 62736: 710,\n 62971: 711,\n 63105: 712,\n 63130: 713,\n 63143: 714,\n 63246: 715,\n 63295: 716,\n 63394: 717,\n 63405: 718,\n 63427: 719,\n 63649: 720,\n 63824: 721,\n 63863: 722,\n 63908: 723,\n 64318: 724,\n 64362: 725,\n 64490: 726,\n 64510: 727,\n 64575: 728,\n 64630: 729,\n 64637: 730,\n 64724: 731,\n 64797: 732,\n 64967: 733,\n 64981: 734,\n 65248: 735,\n 65353: 736,\n 65405: 737,\n 65470: 738,\n 65936: 739,\n 66084: 740,\n 66236: 741,\n 66399: 742,\n 66528: 743,\n 66593: 744,\n 66840: 745,\n 66963: 746,\n 67092: 747,\n 67111: 748,\n 67140: 749,\n 67202: 750,\n 67276: 751,\n 67327: 752,\n 67333: 753,\n 67386: 754,\n 67435: 755,\n 67561: 756,\n 67576: 757,\n 67864: 758,\n 67966: 759,\n 68035: 760,\n 68200: 761,\n 68211: 762,\n 68289: 763,\n 68419: 764,\n 68488: 765,\n 68683: 766,\n 68694: 767,\n 68743: 768,\n 68775: 769,\n 68802: 770,\n 68853: 771,\n 68874: 772,\n 68929: 773,\n 68965: 774,\n 69053: 775,\n 69103: 776,\n 69154: 777,\n 69263: 778,\n 69366: 779,\n 69392: 780,\n 69410: 781,\n 69496: 782,\n 69539: 783,\n 69648: 784,\n 69714: 785,\n 69743: 786,\n 69846: 787,\n 69852: 788,\n 69965: 789,\n 70383: 790,\n 70680: 791,\n 70694: 792,\n 70714: 793,\n 70724: 794,\n 70773: 795,\n 70775: 796,\n 70786: 797,\n 70952: 798,\n 71038: 799,\n 71077: 800,\n 71080: 801,\n 71109: 802,\n 71219: 803,\n 71503: 804,\n 71578: 805,\n 71645: 806,\n 71658: 807,\n 71687: 808,\n 71733: 809,\n 71752: 810,\n 71834: 811,\n 71870: 812,\n 71930: 813,\n 71991: 814,\n 72084: 815,\n 72117: 816,\n 72129: 817,\n 72249: 818,\n 72314: 819,\n 72404: 820,\n 72496: 821,\n 72533: 822,\n 72551: 823,\n 72676: 824,\n 72989: 825,\n 73099: 826,\n 73185: 827,\n 73218: 828,\n 73255: 829,\n 73339: 830,\n 73366: 831,\n 73426: 832,\n 73473: 833,\n 73484: 834,\n 73622: 835,\n 73709: 836,\n 73838: 837,\n 73866: 838,\n 74155: 839,\n 74211: 840,\n 74246: 841,\n 74263: 842,\n 74329: 843,\n 74549: 844,\n 74565: 845,\n 74586: 846,\n 74590: 847,\n 74600: 848,\n 74690: 849,\n 74696: 850,\n 74710: 851,\n 74746: 852,\n 74767: 853,\n 74954: 854,\n 74968: 855,\n 75030: 856,\n 75075: 857,\n 75306: 858,\n 75317: 859,\n 75428: 860,\n 75437: 861,\n 75492: 862,\n 75501: 863,\n 75617: 864,\n 75663: 865,\n 75678: 866,\n 75832: 867,\n 75886: 868,\n 75931: 869,\n 75956: 870,\n 76068: 871,\n 76222: 872,\n 76373: 873,\n 76446: 874,\n 76466: 875,\n 76806: 876,\n 76822: 877,\n 77200: 878,\n 77268: 879,\n 77298: 880,\n 77304: 881,\n 77331: 882,\n 77371: 883,\n 77429: 884,\n 77732: 885,\n 77841: 886,\n 77910: 887,\n 78184: 888,\n 78250: 889,\n 78447: 890,\n 78622: 891,\n 78941: 892,\n 79088: 893,\n 79112: 894,\n 79157: 895,\n 79412: 896,\n 79532: 897,\n 79663: 898,\n 79887: 899,\n 79971: 900,\n 80122: 901,\n 80135: 902,\n 80216: 903,\n 80269: 904,\n 80313: 905,\n 80316: 906,\n 80430: 907,\n 80810: 908,\n 80978: 909,\n 80980: 910,\n 81097: 911,\n 81220: 912,\n 81254: 913,\n 81331: 914,\n 81444: 915,\n 81462: 916,\n 81610: 917,\n 81726: 918,\n 82028: 919,\n 82162: 920,\n 82183: 921,\n 82504: 922,\n 82530: 923,\n 82566: 924,\n 82665: 925,\n 82769: 926,\n 82792: 927,\n 82806: 928,\n 82874: 929,\n 83072: 930,\n 83403: 931,\n 83445: 932,\n 83533: 933,\n 83580: 934,\n 83625: 935,\n 83720: 936,\n 83725: 937,\n 83773: 938,\n 83848: 939,\n 83897: 940,\n 84022: 941,\n 84072: 942,\n 84210: 943,\n 84219: 944,\n 84278: 945,\n 84395: 946,\n 84459: 947,\n 84518: 948,\n 84631: 949,\n 84649: 950,\n 84697: 951,\n 84886: 952,\n 84896: 953,\n 84907: 954,\n 84927: 955,\n 84967: 956,\n 85006: 957,\n 85074: 958,\n 85089: 959,\n 85095: 960,\n 85098: 961,\n 85175: 962,\n 85207: 963,\n 85251: 964,\n 85303: 965,\n 85350: 966,\n 85407: 967,\n 85465: 968,\n 85478: 969,\n 85511: 970,\n 85571: 971,\n 85652: 972,\n 85658: 973,\n 85687: 974,\n 85688: 975,\n 85700: 976,\n 85949: 977,\n 85972: 978,\n 86025: 979,\n 86104: 980,\n 86108: 981,\n 86251: 982,\n 86310: 983,\n 86346: 984,\n 86469: 985,\n 86524: 986,\n 86571: 987,\n 86583: 988,\n 86614: 989,\n 86724: 990,\n 86927: 991,\n 87090: 992,\n 87093: 993,\n 87190: 994,\n 87232: 995,\n 87242: 996,\n 87304: 997,\n 87399: 998,\n 87478: 999,\n ...}"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_session_mapping"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "array([    0,     1,     2, ..., 49997, 49998, 49999])"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_test_sessions_arr = get_mapped_sessions_to_recommend(test_sessions_df, test_session_mapping)\n",
    "mapped_test_sessions_arr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "array([    2,     3,     7, ..., 28141, 28142, 28143], dtype=int64)"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendable_items = candidate_items_df['item_id'].values\n",
    "items_to_ignore = get_items_to_exclude(item_features_df, recommendable_items)\n",
    "items_to_ignore"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "[0,\n 1,\n 3,\n 6,\n 7,\n 8,\n 9,\n 10,\n 11,\n 12,\n 15,\n 16,\n 17,\n 19,\n 20,\n 21,\n 22,\n 23,\n 25,\n 26,\n 27,\n 28,\n 30,\n 31,\n 32,\n 33,\n 34,\n 35,\n 36,\n 37,\n 39,\n 40,\n 42,\n 43,\n 44,\n 45,\n 46,\n 47,\n 48,\n 49,\n 51,\n 53,\n 56,\n 57,\n 58,\n 59,\n 60,\n 61,\n 62,\n 64,\n 65,\n 66,\n 67,\n 68,\n 69,\n 70,\n 71,\n 73,\n 75,\n 76,\n 77,\n 78,\n 79,\n 80,\n 81,\n 82,\n 84,\n 86,\n 87,\n 88,\n 90,\n 91,\n 92,\n 93,\n 94,\n 95,\n 96,\n 97,\n 98,\n 99,\n 100,\n 101,\n 102,\n 104,\n 105,\n 106,\n 107,\n 108,\n 109,\n 110,\n 111,\n 112,\n 113,\n 114,\n 115,\n 116,\n 117,\n 118,\n 119,\n 120,\n 121,\n 123,\n 124,\n 125,\n 126,\n 127,\n 128,\n 129,\n 132,\n 133,\n 134,\n 135,\n 136,\n 137,\n 139,\n 140,\n 141,\n 142,\n 143,\n 144,\n 145,\n 147,\n 149,\n 150,\n 151,\n 152,\n 153,\n 154,\n 155,\n 158,\n 159,\n 160,\n 161,\n 163,\n 164,\n 166,\n 169,\n 170,\n 171,\n 172,\n 173,\n 174,\n 175,\n 176,\n 177,\n 178,\n 179,\n 180,\n 182,\n 183,\n 186,\n 187,\n 188,\n 189,\n 191,\n 192,\n 193,\n 194,\n 195,\n 196,\n 198,\n 199,\n 200,\n 201,\n 202,\n 203,\n 204,\n 205,\n 209,\n 210,\n 211,\n 213,\n 214,\n 215,\n 217,\n 218,\n 219,\n 220,\n 221,\n 223,\n 224,\n 225,\n 226,\n 227,\n 229,\n 230,\n 231,\n 232,\n 233,\n 234,\n 236,\n 237,\n 238,\n 239,\n 240,\n 241,\n 243,\n 244,\n 245,\n 246,\n 247,\n 248,\n 250,\n 251,\n 252,\n 253,\n 254,\n 258,\n 259,\n 260,\n 261,\n 263,\n 264,\n 265,\n 266,\n 267,\n 268,\n 269,\n 270,\n 271,\n 273,\n 274,\n 275,\n 276,\n 277,\n 278,\n 280,\n 282,\n 283,\n 285,\n 287,\n 288,\n 289,\n 291,\n 292,\n 293,\n 294,\n 296,\n 297,\n 298,\n 302,\n 303,\n 304,\n 305,\n 306,\n 308,\n 312,\n 313,\n 314,\n 315,\n 316,\n 317,\n 318,\n 320,\n 321,\n 322,\n 323,\n 324,\n 325,\n 326,\n 329,\n 330,\n 331,\n 332,\n 333,\n 334,\n 335,\n 336,\n 337,\n 338,\n 339,\n 340,\n 341,\n 342,\n 343,\n 345,\n 347,\n 349,\n 351,\n 352,\n 354,\n 355,\n 356,\n 357,\n 358,\n 359,\n 360,\n 361,\n 363,\n 365,\n 367,\n 369,\n 371,\n 373,\n 374,\n 375,\n 378,\n 379,\n 381,\n 382,\n 383,\n 384,\n 385,\n 386,\n 387,\n 388,\n 389,\n 391,\n 392,\n 393,\n 394,\n 395,\n 396,\n 397,\n 398,\n 401,\n 403,\n 404,\n 406,\n 407,\n 409,\n 410,\n 411,\n 412,\n 413,\n 416,\n 417,\n 418,\n 420,\n 421,\n 422,\n 423,\n 424,\n 425,\n 426,\n 428,\n 430,\n 431,\n 433,\n 434,\n 435,\n 437,\n 438,\n 439,\n 440,\n 441,\n 443,\n 444,\n 445,\n 446,\n 447,\n 448,\n 449,\n 450,\n 451,\n 454,\n 455,\n 456,\n 457,\n 460,\n 461,\n 462,\n 463,\n 464,\n 465,\n 467,\n 468,\n 469,\n 470,\n 471,\n 472,\n 474,\n 479,\n 480,\n 482,\n 483,\n 484,\n 485,\n 486,\n 487,\n 488,\n 489,\n 494,\n 495,\n 496,\n 497,\n 499,\n 500,\n 501,\n 502,\n 503,\n 504,\n 505,\n 506,\n 507,\n 508,\n 509,\n 510,\n 511,\n 513,\n 514,\n 515,\n 516,\n 517,\n 518,\n 521,\n 522,\n 523,\n 525,\n 526,\n 527,\n 528,\n 530,\n 531,\n 532,\n 533,\n 534,\n 537,\n 541,\n 543,\n 544,\n 545,\n 546,\n 547,\n 548,\n 549,\n 550,\n 552,\n 553,\n 557,\n 559,\n 560,\n 561,\n 562,\n 563,\n 564,\n 568,\n 569,\n 571,\n 572,\n 573,\n 574,\n 575,\n 576,\n 578,\n 579,\n 580,\n 581,\n 583,\n 585,\n 586,\n 587,\n 588,\n 589,\n 590,\n 591,\n 592,\n 593,\n 594,\n 595,\n 596,\n 599,\n 600,\n 601,\n 602,\n 604,\n 607,\n 608,\n 609,\n 610,\n 612,\n 613,\n 614,\n 616,\n 617,\n 620,\n 621,\n 623,\n 624,\n 625,\n 626,\n 627,\n 628,\n 630,\n 631,\n 632,\n 633,\n 634,\n 636,\n 637,\n 639,\n 640,\n 642,\n 644,\n 645,\n 646,\n 647,\n 648,\n 649,\n 650,\n 651,\n 652,\n 653,\n 654,\n 656,\n 657,\n 658,\n 659,\n 660,\n 662,\n 664,\n 665,\n 666,\n 667,\n 669,\n 670,\n 672,\n 674,\n 676,\n 677,\n 678,\n 679,\n 680,\n 681,\n 683,\n 684,\n 685,\n 687,\n 688,\n 689,\n 690,\n 692,\n 693,\n 694,\n 695,\n 696,\n 697,\n 698,\n 699,\n 701,\n 702,\n 703,\n 704,\n 705,\n 706,\n 707,\n 708,\n 709,\n 710,\n 711,\n 712,\n 713,\n 714,\n 715,\n 716,\n 717,\n 718,\n 719,\n 721,\n 722,\n 723,\n 724,\n 726,\n 727,\n 728,\n 730,\n 732,\n 733,\n 735,\n 738,\n 739,\n 740,\n 743,\n 744,\n 746,\n 748,\n 751,\n 753,\n 754,\n 756,\n 760,\n 761,\n 762,\n 763,\n 764,\n 765,\n 766,\n 767,\n 769,\n 770,\n 771,\n 772,\n 773,\n 774,\n 775,\n 776,\n 778,\n 780,\n 781,\n 782,\n 783,\n 784,\n 785,\n 787,\n 788,\n 792,\n 793,\n 795,\n 797,\n 798,\n 799,\n 800,\n 801,\n 802,\n 803,\n 804,\n 805,\n 806,\n 807,\n 808,\n 811,\n 813,\n 814,\n 815,\n 816,\n 817,\n 818,\n 819,\n 821,\n 822,\n 823,\n 824,\n 826,\n 827,\n 828,\n 829,\n 830,\n 832,\n 833,\n 834,\n 835,\n 836,\n 838,\n 839,\n 840,\n 841,\n 843,\n 846,\n 847,\n 848,\n 850,\n 852,\n 853,\n 854,\n 856,\n 858,\n 860,\n 861,\n 862,\n 863,\n 864,\n 865,\n 867,\n 868,\n 869,\n 872,\n 873,\n 874,\n 875,\n 878,\n 880,\n 881,\n 882,\n 883,\n 885,\n 886,\n 887,\n 889,\n 890,\n 891,\n 892,\n 893,\n 895,\n 896,\n 897,\n 898,\n 899,\n 900,\n 902,\n 903,\n 904,\n 905,\n 906,\n 907,\n 908,\n 909,\n 911,\n 913,\n 914,\n 915,\n 916,\n 917,\n 920,\n 925,\n 926,\n 928,\n 929,\n 932,\n 934,\n 935,\n 936,\n 937,\n 940,\n 941,\n 942,\n 943,\n 944,\n 945,\n 946,\n 947,\n 948,\n 950,\n 953,\n 955,\n 956,\n 957,\n 959,\n 960,\n 961,\n 962,\n 963,\n 964,\n 965,\n 966,\n 967,\n 968,\n 969,\n 970,\n 971,\n 972,\n 974,\n 975,\n 976,\n 978,\n 979,\n 981,\n 982,\n 983,\n 984,\n 985,\n 986,\n 988,\n 989,\n 991,\n 992,\n 993,\n 994,\n 995,\n 996,\n 997,\n 998,\n 999,\n 1000,\n 1001,\n 1003,\n 1004,\n 1005,\n 1006,\n 1007,\n 1008,\n 1010,\n 1011,\n 1012,\n 1013,\n 1018,\n 1021,\n 1022,\n 1023,\n 1024,\n 1025,\n 1026,\n 1027,\n 1029,\n 1030,\n 1031,\n 1032,\n 1034,\n 1035,\n 1036,\n 1038,\n 1039,\n 1040,\n 1041,\n 1042,\n 1043,\n 1044,\n 1046,\n 1047,\n 1049,\n 1050,\n 1051,\n 1052,\n 1054,\n 1055,\n 1056,\n 1057,\n 1058,\n 1059,\n 1060,\n 1061,\n 1063,\n 1064,\n 1066,\n 1067,\n 1068,\n 1069,\n 1070,\n 1071,\n 1072,\n 1073,\n 1075,\n 1076,\n 1077,\n 1078,\n 1079,\n 1080,\n 1081,\n 1082,\n 1083,\n 1084,\n 1085,\n 1086,\n 1087,\n 1088,\n 1089,\n 1090,\n 1091,\n 1092,\n 1093,\n 1094,\n 1096,\n 1097,\n 1098,\n 1099,\n 1100,\n 1103,\n 1104,\n 1105,\n 1106,\n 1109,\n 1111,\n 1113,\n 1114,\n 1115,\n 1116,\n 1117,\n 1118,\n 1119,\n 1120,\n 1121,\n 1122,\n 1123,\n 1124,\n 1125,\n 1128,\n 1130,\n 1131,\n 1133,\n 1134,\n 1136,\n 1137,\n 1138,\n 1140,\n 1141,\n 1144,\n 1145,\n 1147,\n 1148,\n 1149,\n 1150,\n 1151,\n 1153,\n 1154,\n 1155,\n 1156,\n 1157,\n 1158,\n 1159,\n 1160,\n 1161,\n 1162,\n 1164,\n 1165,\n 1166,\n 1167,\n 1168,\n 1169,\n 1171,\n 1174,\n 1176,\n 1177,\n 1178,\n 1179,\n 1180,\n 1181,\n 1182,\n 1184,\n 1186,\n 1187,\n 1188,\n 1189,\n 1190,\n 1191,\n 1192,\n 1193,\n 1194,\n 1195,\n 1198,\n 1199,\n 1200,\n 1201,\n 1202,\n 1203,\n 1204,\n 1205,\n 1207,\n 1208,\n 1209,\n 1210,\n 1211,\n 1212,\n 1213,\n 1214,\n 1215,\n 1218,\n 1219,\n 1220,\n 1221,\n 1222,\n 1223,\n 1224,\n 1226,\n 1228,\n 1229,\n 1231,\n 1233,\n 1234,\n 1235,\n 1236,\n 1237,\n 1238,\n 1239,\n 1240,\n 1241,\n 1243,\n 1244,\n 1246,\n 1247,\n 1248,\n 1249,\n 1250,\n 1251,\n 1252,\n 1253,\n 1254,\n 1255,\n 1256,\n 1258,\n 1260,\n 1261,\n 1262,\n 1263,\n 1264,\n 1265,\n 1266,\n 1267,\n 1269,\n 1270,\n 1272,\n 1273,\n 1274,\n 1275,\n 1276,\n 1277,\n 1278,\n 1279,\n 1280,\n 1281,\n 1283,\n 1285,\n 1286,\n 1287,\n 1288,\n 1289,\n 1290,\n 1291,\n 1292,\n 1294,\n 1295,\n 1296,\n ...]"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_items_to_ignore = [item_mapping[item] for item in items_to_ignore]\n",
    "mapped_items_to_ignore"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "         session_id  item_id                     date  score\n0                 0     8122  2020-12-18 21:19:48.093    0.2\n1                 0     8122  2020-12-18 21:25:00.373    0.2\n2                 0    12704  2020-12-18 21:26:47.986    1.0\n3                 1    13198  2020-03-13 19:35:27.136    0.2\n4                 1    15717  2020-03-13 19:36:15.507    1.0\n...             ...      ...                      ...    ...\n5743815      999999    16481   2020-10-30 23:37:09.46    0.2\n5743816      999999    17238  2020-10-30 23:37:20.658    0.2\n5743817      999999    23434  2020-10-30 23:39:55.186    0.2\n5743818      999999    17271  2020-10-30 23:40:28.149    0.2\n5743819      999999    14028  2020-10-30 23:46:05.218    1.0\n\n[5743820 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>session_id</th>\n      <th>item_id</th>\n      <th>date</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>8122</td>\n      <td>2020-12-18 21:19:48.093</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>8122</td>\n      <td>2020-12-18 21:25:00.373</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>12704</td>\n      <td>2020-12-18 21:26:47.986</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>13198</td>\n      <td>2020-03-13 19:35:27.136</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>15717</td>\n      <td>2020-03-13 19:36:15.507</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5743815</th>\n      <td>999999</td>\n      <td>16481</td>\n      <td>2020-10-30 23:37:09.46</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>5743816</th>\n      <td>999999</td>\n      <td>17238</td>\n      <td>2020-10-30 23:37:20.658</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>5743817</th>\n      <td>999999</td>\n      <td>23434</td>\n      <td>2020-10-30 23:39:55.186</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>5743818</th>\n      <td>999999</td>\n      <td>17271</td>\n      <td>2020-10-30 23:40:28.149</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>5743819</th>\n      <td>999999</td>\n      <td>14028</td>\n      <td>2020-10-30 23:46:05.218</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5743820 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_df['session_id'] = train_set_df['session_id'].map(train_session_mapping)\n",
    "train_set_df['item_id'] = train_set_df['item_id'].map(item_mapping)\n",
    "train_set_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "        session_id  item_id                     date  score\n0                0    16190  2021-06-16 09:53:54.158    0.2\n1                1    14419  2021-06-25 12:23:40.811    0.2\n2                1    14419  2021-06-25 12:24:36.631    0.2\n3                1     6792  2021-06-25 12:24:41.677    0.2\n4                1     3995  2021-06-25 12:24:50.692    0.2\n...            ...      ...                      ...    ...\n229349       49998    21877   2021-06-11 10:22:57.47    0.2\n229350       49998    10263  2021-06-11 10:23:00.663    0.2\n229351       49999     1734  2021-06-30 11:42:15.073    0.2\n229352       49999     1734  2021-06-30 11:43:13.725    0.2\n229353       49999     5705  2021-06-30 11:44:52.704    0.2\n\n[229354 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>session_id</th>\n      <th>item_id</th>\n      <th>date</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>16190</td>\n      <td>2021-06-16 09:53:54.158</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>14419</td>\n      <td>2021-06-25 12:23:40.811</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>14419</td>\n      <td>2021-06-25 12:24:36.631</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>6792</td>\n      <td>2021-06-25 12:24:41.677</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>3995</td>\n      <td>2021-06-25 12:24:50.692</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>229349</th>\n      <td>49998</td>\n      <td>21877</td>\n      <td>2021-06-11 10:22:57.47</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>229350</th>\n      <td>49998</td>\n      <td>10263</td>\n      <td>2021-06-11 10:23:00.663</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>229351</th>\n      <td>49999</td>\n      <td>1734</td>\n      <td>2021-06-30 11:42:15.073</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>229352</th>\n      <td>49999</td>\n      <td>1734</td>\n      <td>2021-06-30 11:43:13.725</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>229353</th>\n      <td>49999</td>\n      <td>5705</td>\n      <td>2021-06-30 11:44:52.704</td>\n      <td>0.2</td>\n    </tr>\n  </tbody>\n</table>\n<p>229354 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sessions_df['session_id'] = test_sessions_df['session_id'].map(test_session_mapping)\n",
    "test_sessions_df['item_id'] = test_sessions_df['item_id'].map(item_mapping)\n",
    "test_sessions_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "        session_id  item_id                     date\n0                0    16190  2021-06-16 09:53:54.158\n1                1    14419  2021-06-25 12:23:40.811\n2                1    14419  2021-06-25 12:24:36.631\n3                1     6792  2021-06-25 12:24:41.677\n4                1     3995  2021-06-25 12:24:50.692\n...            ...      ...                      ...\n229349       49998    21877   2021-06-11 10:22:57.47\n229350       49998    10263  2021-06-11 10:23:00.663\n229351       49999     1734  2021-06-30 11:42:15.073\n229352       49999     1734  2021-06-30 11:43:13.725\n229353       49999     5705  2021-06-30 11:44:52.704\n\n[229354 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>session_id</th>\n      <th>item_id</th>\n      <th>date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>16190</td>\n      <td>2021-06-16 09:53:54.158</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>14419</td>\n      <td>2021-06-25 12:23:40.811</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>14419</td>\n      <td>2021-06-25 12:24:36.631</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>6792</td>\n      <td>2021-06-25 12:24:41.677</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>3995</td>\n      <td>2021-06-25 12:24:50.692</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>229349</th>\n      <td>49998</td>\n      <td>21877</td>\n      <td>2021-06-11 10:22:57.47</td>\n    </tr>\n    <tr>\n      <th>229350</th>\n      <td>49998</td>\n      <td>10263</td>\n      <td>2021-06-11 10:23:00.663</td>\n    </tr>\n    <tr>\n      <th>229351</th>\n      <td>49999</td>\n      <td>1734</td>\n      <td>2021-06-30 11:42:15.073</td>\n    </tr>\n    <tr>\n      <th>229352</th>\n      <td>49999</td>\n      <td>1734</td>\n      <td>2021-06-30 11:43:13.725</td>\n    </tr>\n    <tr>\n      <th>229353</th>\n      <td>49999</td>\n      <td>5705</td>\n      <td>2021-06-30 11:44:52.704</td>\n    </tr>\n  </tbody>\n</table>\n<p>229354 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sessions_df_copy['session_id'] = test_sessions_df_copy['session_id'].map(test_session_mapping)\n",
    "test_sessions_df_copy['item_id'] = test_sessions_df_copy['item_id'].map(item_mapping)\n",
    "test_sessions_df_copy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "data": {
      "text/plain": "          session_id  item_id  RP3betaRecommender_score  \\\n0                  0     2735                  0.014295   \n1                  0     4520                  0.006539   \n2                  0    22360                  0.004120   \n3                  0    17349                  0.004061   \n4                  0     1843                  0.003378   \n...              ...      ...                       ...   \n11215602       49999    23304                  0.000000   \n11215603       49999     1932                  0.000000   \n11215604       49999      794                  0.000000   \n11215605       49999    17810                  0.000000   \n11215606       49999     8015                  0.000000   \n\n          TopPopRecommender_score  ItemKNNCFRecommender_score  \n0                           193.0                    0.022694  \n1                           235.0                    0.012915  \n2                           479.0                    0.004718  \n3                            85.0                    0.004133  \n4                           197.0                    0.005885  \n...                           ...                         ...  \n11215602                   1021.0                    0.001032  \n11215603                    698.0                    0.001028  \n11215604                     43.0                    0.001023  \n11215605                   1483.0                    0.001015  \n11215606                    828.0                    0.001008  \n\n[11215607 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>session_id</th>\n      <th>item_id</th>\n      <th>RP3betaRecommender_score</th>\n      <th>TopPopRecommender_score</th>\n      <th>ItemKNNCFRecommender_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2735</td>\n      <td>0.014295</td>\n      <td>193.0</td>\n      <td>0.022694</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>4520</td>\n      <td>0.006539</td>\n      <td>235.0</td>\n      <td>0.012915</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>22360</td>\n      <td>0.004120</td>\n      <td>479.0</td>\n      <td>0.004718</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>17349</td>\n      <td>0.004061</td>\n      <td>85.0</td>\n      <td>0.004133</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1843</td>\n      <td>0.003378</td>\n      <td>197.0</td>\n      <td>0.005885</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11215602</th>\n      <td>49999</td>\n      <td>23304</td>\n      <td>0.000000</td>\n      <td>1021.0</td>\n      <td>0.001032</td>\n    </tr>\n    <tr>\n      <th>11215603</th>\n      <td>49999</td>\n      <td>1932</td>\n      <td>0.000000</td>\n      <td>698.0</td>\n      <td>0.001028</td>\n    </tr>\n    <tr>\n      <th>11215604</th>\n      <td>49999</td>\n      <td>794</td>\n      <td>0.000000</td>\n      <td>43.0</td>\n      <td>0.001023</td>\n    </tr>\n    <tr>\n      <th>11215605</th>\n      <td>49999</td>\n      <td>17810</td>\n      <td>0.000000</td>\n      <td>1483.0</td>\n      <td>0.001015</td>\n    </tr>\n    <tr>\n      <th>11215606</th>\n      <td>49999</td>\n      <td>8015</td>\n      <td>0.000000</td>\n      <td>828.0</td>\n      <td>0.001008</td>\n    </tr>\n  </tbody>\n</table>\n<p>11215607 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_test_df = pd.read_parquet('../../Dataset/xgb_candidates/candidates_test_df.parquet', engine='pyarrow')\n",
    "candidates_test_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "inv_item_map = {v: k for k, v in item_mapping.items()}\n",
    "inv_session_map = {v: k for k, v in test_session_mapping.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "data": {
      "text/plain": "          session_id  item_id  RP3betaRecommender_score  \\\n0                 26     3260                  0.014295   \n1                 26     5383                  0.006539   \n2                 26    26538                  0.004120   \n3                 26    20541                  0.004061   \n4                 26     2213                  0.003378   \n...              ...      ...                       ...   \n11215602     4439757    27695                  0.000000   \n11215603     4439757     2325                  0.000000   \n11215604     4439757      953                  0.000000   \n11215605     4439757    21100                  0.000000   \n11215606     4439757     9524                  0.000000   \n\n          TopPopRecommender_score  ItemKNNCFRecommender_score  \n0                           193.0                    0.022694  \n1                           235.0                    0.012915  \n2                           479.0                    0.004718  \n3                            85.0                    0.004133  \n4                           197.0                    0.005885  \n...                           ...                         ...  \n11215602                   1021.0                    0.001032  \n11215603                    698.0                    0.001028  \n11215604                     43.0                    0.001023  \n11215605                   1483.0                    0.001015  \n11215606                    828.0                    0.001008  \n\n[11215607 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>session_id</th>\n      <th>item_id</th>\n      <th>RP3betaRecommender_score</th>\n      <th>TopPopRecommender_score</th>\n      <th>ItemKNNCFRecommender_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>26</td>\n      <td>3260</td>\n      <td>0.014295</td>\n      <td>193.0</td>\n      <td>0.022694</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>26</td>\n      <td>5383</td>\n      <td>0.006539</td>\n      <td>235.0</td>\n      <td>0.012915</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26</td>\n      <td>26538</td>\n      <td>0.004120</td>\n      <td>479.0</td>\n      <td>0.004718</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>26</td>\n      <td>20541</td>\n      <td>0.004061</td>\n      <td>85.0</td>\n      <td>0.004133</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>26</td>\n      <td>2213</td>\n      <td>0.003378</td>\n      <td>197.0</td>\n      <td>0.005885</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11215602</th>\n      <td>4439757</td>\n      <td>27695</td>\n      <td>0.000000</td>\n      <td>1021.0</td>\n      <td>0.001032</td>\n    </tr>\n    <tr>\n      <th>11215603</th>\n      <td>4439757</td>\n      <td>2325</td>\n      <td>0.000000</td>\n      <td>698.0</td>\n      <td>0.001028</td>\n    </tr>\n    <tr>\n      <th>11215604</th>\n      <td>4439757</td>\n      <td>953</td>\n      <td>0.000000</td>\n      <td>43.0</td>\n      <td>0.001023</td>\n    </tr>\n    <tr>\n      <th>11215605</th>\n      <td>4439757</td>\n      <td>21100</td>\n      <td>0.000000</td>\n      <td>1483.0</td>\n      <td>0.001015</td>\n    </tr>\n    <tr>\n      <th>11215606</th>\n      <td>4439757</td>\n      <td>9524</td>\n      <td>0.000000</td>\n      <td>828.0</td>\n      <td>0.001008</td>\n    </tr>\n  </tbody>\n</table>\n<p>11215607 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_test_df['session_id'] = candidates_test_df['session_id'].map(inv_session_map)\n",
    "candidates_test_df['item_id'] = candidates_test_df['item_id'].map(inv_item_map)\n",
    "candidates_test_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "data": {
      "text/plain": "          session_id  item_id  RP3betaRecommender_score  \\\n2318              10    10220                       0.0   \n2323              10     4681                       0.0   \n2363              10     4557                       0.0   \n9360              41     4557                       0.0   \n9406              41     4681                       0.0   \n...              ...      ...                       ...   \n18409794       81595    10220                       0.0   \n18410499       81598     4681                       0.0   \n18413536       81612    10220                       0.0   \n18413546       81612     4681                       0.0   \n18413584       81612     4557                       0.0   \n\n          TopPopRecommender_score  ItemKNNCFRecommender_score  target  is_fake  \n2318                          0.0                         0.0       0    False  \n2323                          0.0                         0.0       0    False  \n2363                          0.0                         0.0       0    False  \n9360                          0.0                         0.0       0    False  \n9406                          0.0                         0.0       0    False  \n...                           ...                         ...     ...      ...  \n18409794                      0.0                         0.0       0    False  \n18410499                      0.0                         0.0       0    False  \n18413536                      0.0                         0.0       0    False  \n18413546                      0.0                         0.0       0    False  \n18413584                      0.0                         0.0       0    False  \n\n[26830 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>session_id</th>\n      <th>item_id</th>\n      <th>RP3betaRecommender_score</th>\n      <th>TopPopRecommender_score</th>\n      <th>ItemKNNCFRecommender_score</th>\n      <th>target</th>\n      <th>is_fake</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2318</th>\n      <td>10</td>\n      <td>10220</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2323</th>\n      <td>10</td>\n      <td>4681</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2363</th>\n      <td>10</td>\n      <td>4557</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>9360</th>\n      <td>41</td>\n      <td>4557</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>9406</th>\n      <td>41</td>\n      <td>4681</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>18409794</th>\n      <td>81595</td>\n      <td>10220</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>18410499</th>\n      <td>81598</td>\n      <td>4681</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>18413536</th>\n      <td>81612</td>\n      <td>10220</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>18413546</th>\n      <td>81612</td>\n      <td>4681</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>18413584</th>\n      <td>81612</td>\n      <td>4557</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>26830 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_candidates_train_df = pd.read_parquet('../../Dataset/xgb_candidates/true_candidates_train_df.parquet', engine='pyarrow')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "662236ff4a3599a6369637d46054e6465a367b5e2f2cb09ee0460010b626acba"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('recsys')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}